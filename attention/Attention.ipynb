{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "dd195f96-0d60-400c-c45f-b036e115b8c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur Baia\n"
          ]
        }
      ],
      "source": [
        "nome = 'Arthur Baia'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Este exercício é similar ao da aula 4, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 80):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente mas fácil de entender) e outra matricial (eficiente mas difícil de entender).\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w9f3PfifAwpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 28 14:35:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   54C    P0    22W /  N/A |   1349MiB /  5944MiB |     26%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1952      G   /usr/lib/xorg/Xorg                 70MiB |\n",
            "|    0   N/A  N/A      4083      G   /usr/lib/xorg/Xorg                194MiB |\n",
            "|    0   N/A  N/A      4262      G   /usr/bin/gnome-shell               51MiB |\n",
            "|    0   N/A  N/A      4725      G   /usr/lib/firefox/firefox          169MiB |\n",
            "|    0   N/A  N/A      6892      G   ...572847625700112130,131072       22MiB |\n",
            "|    0   N/A  N/A     15961      C   /bin/python3                      743MiB |\n",
            "|    0   N/A  N/A     41411      G   ...RendererForSitePerProcess       83MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "whTCe2i7AtoV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2wbnfzst5O3k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0HIN_xLI_TuT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "I am sad that a period of history that is so fascinating and so rich in material for film can be mad\n",
            "THE SUNSHINE BOYS was the hilarious 1975 screen adaptation of Neil Simon's play about a retired vaud\n",
            "this has by far been one of the most beautiful portraits of a person that I've ever seen on screen. \n",
            "3 últimas amostras treino:\n",
            "My daughter gets really put out at me when I refer to Drew Barrymore as looking as if she'd been hit\n",
            "This is quite a dull movie. Well-shot with realistic performances especially a very good one from De\n",
            "Too bad neither the animals or Eddie Murphy had anything to say worth saying. this movie is just bla\n",
            "3 primeiras amostras validação:\n",
            "The title should have been the walker. The guy expend 90% of the movie walking. He doesn't know what\n",
            "The movie was a long awaited release, which where a bit disappointing because of the expectation's I\n",
            "The jokes are obvious, the gags are corny, and the characters are walking characatures - but I could\n",
            "3 últimas amostras validação:\n",
            "I watched this film over a hundred times. It is really best Serbian movie made ever.I wood like to r\n",
            "To put it simply, The Fan was a disappointment. It felt like as if I was watching Taxi Driver, excep\n",
            "Lindsay Anderson was very much a European film maker , whereas the likes of David Lean , Ridley Scot\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "unk = '<UNK>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self, max_vocab_token=3000):\n",
        "\n",
        "        self.max_vocab_tokens = max_vocab_token\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        # Escreva aqui seu código.\n",
        "        return [self.vocab[word] if word in self.vocab else unk for word in self.tokenize(text)]\n",
        "\n",
        "    def decode(self, tokens: List[int]):\n",
        "        # Escreva aqui seu código.\n",
        "        return ' '.join([self.vocab_inv[token] for token in tokens])\n",
        "\n",
        "    def create_vocab(self, texts: List[str]):\n",
        "        L = [word for phrase in list(map(self.tokenize, texts))\n",
        "             for word in phrase]\n",
        "        k = self.max_vocab_tokens\n",
        "        def vocab(L, k): return {value: key for key, value in enumerate(\n",
        "            dict(collections.Counter(L).most_common(k)))}\n",
        "        self.vocab = vocab(L, k)\n",
        "        self.vocab_inv = {v: k for k, v in self.vocab.items()}\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\"\n",
        "        Convert string to a list of tokens (i.e., words).\n",
        "        This function lower cases everything and removes punctuation.\n",
        "        \"\"\"\n",
        "        # Escreva aqui seu código.\n",
        "        text = re.sub(re.escape('<br /><br />'), '°', text)\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_tokenizer():\n",
        "    phrase = 'a cat walks in the bad.'\n",
        "    assert Tokenizer().tokenize(phrase) == ['a', 'cat', 'walks', 'in', 'the', 'bad', '.']\n",
        "    tokenizer = Tokenizer(len(phrase.split())+1)\n",
        "    vocab_ = tokenizer.create_vocab([phrase])\n",
        "    assert tokenizer.vocab == {'a': 0, 'cat': 1,\n",
        "                               'walks': 2, 'in': 3, 'the': 4, 'bad': 5, '.': 6}\n",
        "    assert tokenizer.vocab_inv == {\n",
        "        0: 'a', 1: 'cat', 2: 'walks', 3: 'in', 4: 'the', 5: 'bad', 6: '.'}\n",
        "    assert tokenizer.encode(phrase) == [0, 1, 2, 3, 4, 5, 6]\n",
        "    assert tokenizer.encode('a cat') == [0, 1]\n",
        "    assert tokenizer.encode('dog') == [unk]\n",
        "    phrase = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    tokenizer = Tokenizer(20)\n",
        "    vocab_ = tokenizer.create_vocab(phrase)\n",
        "    assert tokenizer.vocab == {'a': 0,\n",
        "                               'walks': 1,\n",
        "                               'in': 2,\n",
        "                               'the': 3,\n",
        "                               '.': 4,\n",
        "                               'cat': 5,\n",
        "                               'bad': 6,\n",
        "                               'dog': 7,\n",
        "                               'good': 8}\n",
        "\n",
        "\n",
        "test_tokenizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBdataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, corpus: List[str], tokenizer, context_size=9):\n",
        "        data = []\n",
        "        for text in corpus:\n",
        "            tokens = tokenizer.encode(text)\n",
        "            data.extend([\n",
        "                tokens[i:i+context_size+1] for i in range(len(tokens)-context_size)\n",
        "                if unk not in tokens[i:i+context_size+1] and len(tokens) > context_size\n",
        "            ])\n",
        "        self.data = torch.tensor(data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx][:-1], self.data[idx][-1].long()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to test the dataset\"\"\"\n",
        "def test_dataset():\n",
        "    vocab = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    vocab.create_vocab(test_corpus)\n",
        "    unseen_tokens = ['aprendendo sobre o modelo n-grama']\n",
        "    dataset_ = IMDBdataset(unseen_tokens, vocab, 3)\n",
        "    assert len(dataset_) == 0\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, vocab, context_size=context_size)\n",
        "    assert len(dataset_) == 8\n",
        "    assert len(dataset_.__getitem__(0)[0].tolist()) == context_size\n",
        "    assert len([dataset_.__getitem__(0)[1].tolist()]) == 1\n",
        "    assert dataset_.__getitem__(0)[1].tolist() == vocab.encode('in')[0]\n",
        "    assert dataset_.__getitem__(0)[0].tolist() == vocab.encode('a cat walks')\n",
        "    assert dataset_.__getitem__(1)[0].tolist() == vocab.encode('cat walks in')\n",
        "    assert dataset_.__getitem__(1)[1].tolist() == vocab.encode('the')[0]\n",
        "\n",
        "\n",
        "test_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"function to test the dataloader\"\"\"\n",
        "def test_dataloader_():\n",
        "    tokenizer = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    tokenizer.create_vocab(test_corpus)\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, tokenizer, context_size=context_size)\n",
        "    dataloader_ = torch.utils.data.DataLoader(dataset_, batch_size=2, shuffle=True)\n",
        "    for batch in dataloader_:\n",
        "        assert len(batch[0].shape) == 2\n",
        "        assert len(batch[1].shape) == 1\n",
        "        assert batch[0].shape[0] == batch[1].shape[0]\n",
        "        assert batch[0].shape[1] == context_size\n",
        "        break\n",
        "test_dataloader_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LoopedSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(LoopedSelfAttention, self).__init__()\n",
        "\n",
        "        self.wq = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.wk= nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.wv = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, seq_len, embedding_dim)\n",
        "        Q = self.wq(x)\n",
        "        K = self.wk(x)\n",
        "        V = self.wv(x)\n",
        "        \n",
        "        new_embeddings = []\n",
        "        for query in Q:\n",
        "            scores = []\n",
        "            for key in K:\n",
        "                score = (query * key).sum()\n",
        "                scores.append(score)\n",
        "\n",
        "            attention_weights = torch.softmax(torch.FloatTensor(scores), dim=0)\n",
        "\n",
        "            new_embedding = 0\n",
        "            for weight, value in zip(attention_weights, V):\n",
        "                new_embedding += weight * value\n",
        "\n",
        "            new_embeddings.append(new_embedding)\n",
        "        return torch.stack(new_embeddings)\n",
        "\n",
        "\n",
        "class MatrixSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(MatrixSelfAttention, self).__init__()\n",
        "\n",
        "        self.wq = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.wk = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.wv = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, seq_len, embedding_dim)\n",
        "        Q = self.wq(x)\n",
        "        K = self.wk(x)\n",
        "        V = self.wv(x)\n",
        "        print('k',K)\n",
        "        print('k t',torch.transpose(K, -2, -1))\n",
        "        scores = torch.matmul(Q, torch.transpose(K, -2, -1))\n",
        "        print(scores)\n",
        "        attention_weigths = torch.softmax(scores, dim=-1)\n",
        "        new_embeddings = torch.matmul(attention_weigths, V)\n",
        "        return new_embeddings\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k tensor([[[ 0.6590,  0.1529],\n",
            "         [ 1.6470, -1.4208]],\n",
            "\n",
            "        [[ 0.0905,  0.9164],\n",
            "         [ 0.6113,  0.0733]]], grad_fn=<AddBackward0>)\n",
            "k t tensor([[[ 0.6590,  1.6470],\n",
            "         [ 0.1529, -1.4208]],\n",
            "\n",
            "        [[ 0.0905,  0.6113],\n",
            "         [ 0.9164,  0.0733]]], grad_fn=<TransposeBackward0>)\n",
            "tensor([[[ 0.2601,  0.0035],\n",
            "         [ 0.1281, -1.9326]],\n",
            "\n",
            "        [[-0.4109, -0.1590],\n",
            "         [-0.0304, -0.3016]]], grad_fn=<UnsafeViewBackward>)\n"
          ]
        }
      ],
      "source": [
        "def test_self_attention():\n",
        "    embedding_dim = 2\n",
        "    seq_len = 2\n",
        "    batch_size = 2\n",
        "    x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "    torch.manual_seed(42)\n",
        "    looped_self_attention = LoopedSelfAttention(embedding_dim)\n",
        "    torch.manual_seed(42)\n",
        "    matrix_self_attention = MatrixSelfAttention(embedding_dim)\n",
        "    # print('shapes: ', looped_self_attention(x).shape, matrix_self_attention(x).shape)\n",
        "    assert looped_self_attention(x).shape == matrix_self_attention(x).shape == (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "test_self_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "looped Self Attention exectution time:\n",
            "303 µs ± 4.98 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
            "matrix Self Attention exectution time:\n",
            "161 µs ± 3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "def time_test():\n",
        "    print('looped Self Attention exectution time:')\n",
        "    %timeit looped_self_attention(x)\n",
        "    print('matrix Self Attention exectution time:')\n",
        "    %timeit matrix_self_attention(x)\n",
        "time_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(AttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(context_size, embedding_dim)*0.001 - 0.0005)\n",
        "        self.self_attention = MatrixSelfAttention(embedding_dim)\n",
        "        self.MLP = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x has shape (batch_size, context_size)\n",
        "        x = self.embedding(x)\n",
        "      \n",
        "        x = x + self.pos_embedding\n",
        "      \n",
        "        x = self.self_attention(x)\n",
        "\n",
        "        x = self.MLP(x)\n",
        "\n",
        "        x = self.output(x[:, -1, :])\n",
        "        \n",
        "        return x\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final shape: torch.Size([2, 100])\n"
          ]
        }
      ],
      "source": [
        "def test_model():\n",
        "    vocab_size = 100\n",
        "    embedding_dim = 10\n",
        "    context_size = 5\n",
        "    model = AttentionModel(vocab_size = vocab_size, embedding_dim = embedding_dim, context_size = context_size)\n",
        "    x = torch.randint(0, vocab_size, (2, context_size))\n",
        "    # print('x shape: ', x.shape)\n",
        "    print('final shape:',model(x).shape)\n",
        "    # assert model(x).shape == (2, vocab_size)\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to train the model, which returns the train loss\"\"\"\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in train_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_func(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to evaluate the model, which returns the validation loss and accuracy\"\"\"\n",
        "def evaluate(model, num_examples, valid_dataloader, loss_func):\n",
        "\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    for x, y in valid_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          logits = model(x)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        loss = loss_func(logits, y)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        correct += (preds==y).sum().item()\n",
        "\n",
        "    return (correct / num_examples), (val_loss / len(valid_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def test_train_eval():\n",
        "    tokenizer = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    tokenizer.create_vocab(test_corpus)\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, tokenizer, context_size=context_size)\n",
        "    dataloader_ = torch.utils.data.DataLoader(dataset_, batch_size=2, shuffle=True)\n",
        "    model = AttentionModel(vocab_size = len(tokenizer.vocab), embedding_dim = 10, context_size = context_size).to(device)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_loss = train(model, dataloader_, loss_func, optimizer)\n",
        "    acc, val_loss = evaluate(model, len(dataset_), dataloader_, loss_func)\n",
        "    assert train_loss > 0\n",
        "    assert acc >= 0\n",
        "    assert val_loss > 0\n",
        "    \n",
        "test_train_eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 0.0001\n",
        "tokenizer = Tokenizer(max_vocab_token=3000)\n",
        "tokenizer.create_vocab(x_train)\n",
        "context_size = 9\n",
        "dataset_train = IMDBdataset(x_train, tokenizer, context_size=context_size)\n",
        "dataset_valid = IMDBdataset(x_valid, tokenizer, context_size=context_size)\n",
        "num_examples = len(dataset_valid)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=True)\n",
        "model = AttentionModel(vocab_size = len(tokenizer.vocab), embedding_dim = 100, context_size = context_size).to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to(device)\n",
        "loss_func.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pré treino; Validation Loss: 8.018; Perplexity: 3035.908; Accuracy: 0.000\n",
            "Epoch: 1; Train Loss: 5.509; Perplexity: 246.830; Validation Loss: 5.195; Accuracy: 0.119\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPré treino; Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m; Perplexity: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mexp(valid_loss)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m; Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   train_loss \u001b[39m=\u001b[39m train(model, train_dataloader, loss_func, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   accuracy, valid_loss \u001b[39m=\u001b[39m evaluate(model, num_examples, valid_dataloader, loss_func)\n",
            "\u001b[1;32m/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb Cell 27\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, loss_func, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:345\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 345\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    348\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    349\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:385\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    384\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    387\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "\u001b[1;32m/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb Cell 27\u001b[0m in \u001b[0;36mIMDBdataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/attention/Aula_5_213259.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[idx][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlong()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "accuracy_train = []\n",
        "perplexities = []\n",
        "accuracy, valid_loss = evaluate(model, num_examples,valid_dataloader, loss_func)\n",
        "print(f\"Pré treino; Validation Loss: {valid_loss:.3f}; Perplexity: {np.exp(valid_loss):.3f}; Accuracy: {accuracy:.3f}\")\n",
        "for t in range(epochs):\n",
        "  train_loss = train(model, train_dataloader, loss_func, optimizer)\n",
        "  train_losses.append(train_loss)\n",
        "  accuracy, valid_loss = evaluate(model, num_examples, valid_dataloader, loss_func)\n",
        "  valid_losses.append(valid_loss)\n",
        "  accuracy_train.append(accuracy)\n",
        "  perplexities.append(np.exp(train_loss))\n",
        "  print(f\"Epoch: {t+1}; Train Loss: {train_loss:.3f}; Perplexity: {np.exp(train_loss):.3f}; Validation Loss: {valid_loss:.3f}; Accuracy: {accuracy:.3f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
