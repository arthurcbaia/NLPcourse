{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Nome: Arthur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "## Instruções\n",
        "\n",
        "Neste colab iremos treinar um modelo T5 para traduzir de inglês para português. Iremos treiná-lo com o data Paracrawl.\n",
        "\n",
        "- Usaremos o dataset Paracrawl Inglês-Português. Truncamos o dataset de treino para apenas 100k pares para deixar o treinamento mais rápido. Quem quiser pode treinar com mais amostras. Se demorar muito para treinar, truncar o dataset ainda mais.\n",
        "\n",
        "- Usaremos o BLEU como métrica. Usaremos o SacreBLEU pois sempre faz o mesmo pré-processamento (tokenização, lowercase). Não usaremos torchnlp.metrics.bleu, torchtext.data.metrics.bleu_score, etc. SacreBLEU é lento: usar poucas amostras de validação (ex: 5k)\n",
        "\n",
        "\n",
        "Usaremos o modelo PTT5 disponível no model hub da HuggingFace:\n",
        "\n",
        "https://huggingface.co/unicamp-dl/ptt5-small-portuguese-vocab\n",
        "\n",
        "Este é  um T5 pré-treinado em textos em português e com tokenizador em português.\n",
        "\n",
        "É recomendável salvar os pesos do modelo e estado dos otimizadores, pois o treinamento é longo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FgW-boJLU0wU"
      },
      "outputs": [],
      "source": [
        "# Configurações gerais\n",
        "model_name = \"unicamp-dl/ptt5-small-portuguese-vocab\"\n",
        "batch_size = 64\n",
        "accumulate_grad_batches = 2\n",
        "source_max_length = 128\n",
        "target_max_length = 128\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0mXaMmG4cb-F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /home/arthur/.local/lib/python3.8/site-packages (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/arthur/.local/lib/python3.8/site-packages (from sacrebleu) (1.22.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/arthur/.local/lib/python3.8/site-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: regex in /home/arthur/.local/lib/python3.8/site-packages (from sacrebleu) (2022.9.13)\n",
            "Requirement already satisfied: lxml in /home/arthur/.local/lib/python3.8/site-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: portalocker in /home/arthur/.local/lib/python3.8/site-packages (from sacrebleu) (2.5.1)\n",
            "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu) (0.4.3)\n",
            "Requirement already satisfied: transformers in /home/arthur/.local/lib/python3.8/site-packages (4.22.2)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (2022.9.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/arthur/.local/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/arthur/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arthur/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/arthur/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/arthur/.local/lib/python3.8/site-packages (from requests->transformers) (2022.9.14)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/arthur/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.10)\n",
            "Requirement already satisfied: sentencepiece in /home/arthur/.local/lib/python3.8/site-packages (0.1.97)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.13.1-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 3.9 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/arthur/.local/lib/python3.8/site-packages (from accelerate) (1.22.3)\n",
            "Requirement already satisfied: torch>=1.4.0 in /home/arthur/.local/lib/python3.8/site-packages (from accelerate) (1.12.1)\n",
            "Requirement already satisfied: pyyaml in /home/arthur/.local/lib/python3.8/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /home/arthur/.local/lib/python3.8/site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/arthur/.local/lib/python3.8/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /home/arthur/.local/lib/python3.8/site-packages (from torch>=1.4.0->accelerate) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/arthur/.local/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (3.0.8)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.13.1\n"
          ]
        }
      ],
      "source": [
        "! pip install sacrebleu\n",
        "! pip install transformers\n",
        "! pip install sentencepiece\n",
        "! pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ob7qL6kUVjbu"
      },
      "outputs": [],
      "source": [
        "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
        "import gzip\n",
        "import os\n",
        "import random\n",
        "import sacrebleu\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, get_scheduler\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from accelerate import Accelerator\n",
        "from numpy import exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bJlZDb1VY29r"
      },
      "outputs": [],
      "source": [
        "# Important: Fix seeds so we can replicate results\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfkvMGl4JA1"
      },
      "source": [
        "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-Co8U6O4Gl3"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2wbnfzst5O3k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘paracrawl_enpt_train.tsv.gz’ already there; not retrieving.\n",
            "\n",
            "File ‘paracrawl_enpt_test.tsv.gz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_train.tsv.gz\n",
        "! wget -nc https://storage.googleapis.com/unicamp-dl/ia024a_2022s2/paracrawl_enpt_test.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (100k pares) e val (5k pares) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0HIN_xLI_TuT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "100000 amostras de treino\n",
            "3 primeiras amostras treino:\n",
            "0: source: More Croatian words and phrases\n",
            "   target: Mais palavras e frases em croata\n",
            "1: source: Jerseys and pullovers, containing at least 50Â % by weight of wool and weighing 600Â g or more per article 6110 11 10 (PCE)\n",
            "   target: Camisolas e pulôveres, com pelo menos 50 %, em peso, de lã e pesando 600g ou mais por unidade 6110 11 10 (PCE)\n",
            "2: source: Atex Colombia SAS makes available its lead product, 100% natural liquid latex, excellent quality and price. ... Welding manizales caldas Colombia a DuckDuckGo\n",
            "   target: Atex Colômbia SAS torna principal produto está disponível, látex líquido 100% natural, excelente qualidade e preço. ...\n",
            "\n",
            "5000 amostras de validação\n",
            "3 primeiras amostras validação:\n",
            "0: source: «You have hidden these things from the wise and the learned you have revealed them to the childlike»\n",
            "   target: «Escondeste estas coisas aos sábios e entendidos e as revelaste aos pequenos»\n",
            "1: source: Repair of computers, application programming, network installations, web design, graphic design, and also the most. Computer consulting in Santa Lucía\n",
            "   target: Reparação de computadores, programação de aplicações, instalações de rede, web design, design gráfico, e também a.\n",
            "2: source: He was born in Fafe (Braga) and he graduated in Law in Coimbra University.\n",
            "   target: É natural de Fafe (Braga) e Licenciado em Direito pela Universidade de Coimbra.\n",
            "\n",
            "20000 amostras de test\n",
            "3 primeiras amostras test:\n",
            "0: source: In this way, the civil life of a nation matures, making it possible for all citizens to enjoy the fruits of genuine tolerance and mutual respect.\n",
            "   target: Deste modo, a vida civil de uma nação amadurece, fazendo com que todos os cidadãos gozem dos frutos da tolerância genuína e do respeito mútuo.\n",
            "1: source: 1999 XIII. Winnipeg, Canada July 23 to August 8\n",
            "   target: 1999 XIII. Winnipeg, Canadá 23 de julho a 8 de agosto\n",
            "2: source: In the mystery of Christmas, Christ's light shines on the earth, spreading, as it were, in concentric circles.\n",
            "   target: No mistério do Natal, a luz de Cristo irradia-se sobre a terra, difundindo-se como círculos concêntricos.\n"
          ]
        }
      ],
      "source": [
        "def load_text_pairs(path):\n",
        "    text_pairs = []\n",
        "    for line in gzip.open(path, mode='rt'):\n",
        "        text_pairs.append(line.strip().split('\\t'))\n",
        "    return text_pairs\n",
        "\n",
        "x_train = load_text_pairs('paracrawl_enpt_train.tsv.gz')\n",
        "x_test = load_text_pairs('paracrawl_enpt_test.tsv.gz')\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/val.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "# Truncamos o dataset para 100k pares de treino e 5k pares de validação.\n",
        "x_val = x_train[100000:105000]\n",
        "x_train = x_train[:100000]\n",
        "\n",
        "for set_name, x in [('treino', x_train), ('validação', x_val), ('test', x_test)]:\n",
        "    print(f'\\n{len(x)} amostras de {set_name}')\n",
        "    print(f'3 primeiras amostras {set_name}:')\n",
        "    for i, (source, target) in enumerate(x[:3]):\n",
        "        print(f'{i}: source: {source}\\n   target: {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXnoYK5YXKgk"
      },
      "source": [
        "Criando Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pLrftKzSPBs_"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OMen-JFKLFCb"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        text_pairs: List[Tuple[str]],\n",
        "        tokenizer,\n",
        "        source_max_length: int = 32,\n",
        "        target_max_length: int = 32,\n",
        "    ):\n",
        "        self.original_source = [text[0] for text in text_pairs]\n",
        "        self.original_target = [text[1] for text in text_pairs]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            self.original_source,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=source_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        targets = tokenizer(\n",
        "            self.original_target,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=target_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        self.text_pairs = [\n",
        "            {\n",
        "                \"source_tokens\": input_ids,\n",
        "                \"source_attention_mask\": input_attention_mask,\n",
        "                \"target_tokens\": target_ids,\n",
        "                \"target_attention_mask\": target_attention_mask,\n",
        "            }\n",
        "            for input_ids, input_attention_mask, target_ids, target_attention_mask in zip(\n",
        "                inputs.input_ids,\n",
        "                inputs.attention_mask,\n",
        "                targets.input_ids,\n",
        "                targets.attention_mask,\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.text_pairs[idx]\n",
        "        (\n",
        "            source_token_ids,\n",
        "            source_mask,\n",
        "            target_token_ids,\n",
        "            target_mask,\n",
        "            original_source,\n",
        "            original_target,\n",
        "        ) = (\n",
        "            data[\"source_tokens\"],\n",
        "            data[\"source_attention_mask\"],\n",
        "            data[\"target_tokens\"],\n",
        "            data[\"target_attention_mask\"],\n",
        "            self.original_source[idx],\n",
        "            self.original_target[idx],\n",
        "        )\n",
        "        return (\n",
        "            source_token_ids,\n",
        "            source_mask,\n",
        "            target_token_ids,\n",
        "            target_mask,\n",
        "            original_source,\n",
        "            original_target,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD"
      },
      "source": [
        "## Testando o DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZoKiQXCvwGrP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source_token_ids:\n",
            " tensor([[  31, 1528, 1079,  634, 1241, 7531,    1]])\n",
            "source_mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
            "target_token_ids:\n",
            " tensor([[2077, 6618,    4, 1241, 7531,    1]])\n",
            "target_mask:\n",
            " tensor([[1, 1, 1, 1, 1, 1]])\n",
            "source_token_ids.shape: torch.Size([1, 7])\n",
            "source_mask.shape: torch.Size([1, 7])\n",
            "target_token_ids.shape: torch.Size([1, 6])\n",
            "target_mask.shape: torch.Size([1, 6])\n"
          ]
        }
      ],
      "source": [
        "text_pairs = [('we like pizza', 'eu gosto de pizza')]\n",
        "dataset_debug = MyDataset(\n",
        "    text_pairs=text_pairs,\n",
        "    tokenizer=tokenizer,\n",
        "    source_max_length=source_max_length,\n",
        "    target_max_length=target_max_length)\n",
        "\n",
        "dataloader_debug = DataLoader(dataset_debug, batch_size=10, shuffle=True, \n",
        "                              num_workers=0)\n",
        "\n",
        "source_token_ids, source_mask, target_token_ids, target_mask, _, _ = next(iter(dataloader_debug))\n",
        "print('source_token_ids:\\n', source_token_ids)\n",
        "print('source_mask:\\n', source_mask)\n",
        "print('target_token_ids:\\n', target_token_ids)\n",
        "print('target_mask:\\n', target_mask)\n",
        "\n",
        "print('source_token_ids.shape:', source_token_ids.shape)\n",
        "print('source_mask.shape:', source_mask.shape)\n",
        "print('target_token_ids.shape:', target_token_ids.shape)\n",
        "print('target_mask.shape:', target_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBptbyTvXBhC"
      },
      "source": [
        "## Criando DataLoaders de Treino/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "i2_Fcs0VXD5W"
      },
      "outputs": [],
      "source": [
        "dataset_train = MyDataset(text_pairs=x_train,\n",
        "                          tokenizer=tokenizer,\n",
        "                          source_max_length=source_max_length,\n",
        "                          target_max_length=target_max_length)\n",
        "\n",
        "dataset_val = MyDataset(text_pairs=x_val,\n",
        "                        tokenizer=tokenizer,\n",
        "                        source_max_length=source_max_length,\n",
        "                        target_max_length=target_max_length)\n",
        "\n",
        "dataset_test = MyDataset(text_pairs=x_test,\n",
        "                         tokenizer=tokenizer,\n",
        "                         source_max_length=source_max_length,\n",
        "                         target_max_length=target_max_length)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, \n",
        "                            num_workers=0)\n",
        "\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=batch_size,\n",
        "                             shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    train_dataloader,\n",
        "    accelerator,\n",
        "    optimizer,\n",
        "    lr_scheduler,\n",
        "    model,\n",
        "):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    progress_bar = tqdm(range(len(train_dataloader)))\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        train_loss += loss.item()\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        if step % 8 == 0:\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    return train_loss / len(train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(\n",
        "    model,\n",
        "    val_dataloader\n",
        "):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    progress_bar = tqdm(range(len(val_dataloader)))\n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    return val_loss / len(val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_train_epochs = 1\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fp16=True)\n",
        "device = accelerator.device\n",
        "\n",
        "model, optimizer, lr_scheduler, train_dataloader, val_dataloader = accelerator.prepare(\n",
        "    model, optimizer, lr_scheduler, train_dataloader, val_dataloader\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_81918/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3482448087.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 4&gt;</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_81918/3482448087.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_81918/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">88541830.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_81918/88541830.py'</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'module'</span> object is not callable\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_81918/\u001b[0m\u001b[1;33m3482448087.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<cell line: 4>\u001b[0m                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_81918/3482448087.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_81918/\u001b[0m\u001b[1;33m88541830.py\u001b[0m:\u001b[94m11\u001b[0m in \u001b[92mtrain\u001b[0m                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_81918/88541830.py'\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'module'\u001b[0m object is not callable\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "perplexities = []\n",
        "for epoch in range(num_train_epochs):\n",
        "    train_loss = train('train_dataloader', 'accelerator', 'optimizer', 'lr_scheduler', model) \n",
        "    # train_losses.append(train_loss)\n",
        "    # valid_loss = evaluate_model(model, val_dataloader)\n",
        "    # valid_losses.append(valid_loss)\n",
        "    # perplexities.append(exp(train_loss))\n",
        "    # print(\n",
        "    #     f\"Epoch: {epoch+1}; Train Loss: {train_loss:.3f}; Perplexity: {exp(train_loss):.3f}; Validation Loss: {valid_loss:.3f};\"\n",
        "    # )\n",
        "\n",
        "# Save and upload\n",
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained('.', save_function=accelerator.save)\n",
        "if accelerator.is_main_process:\n",
        "    tokenizer.save_pretrained('.')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
