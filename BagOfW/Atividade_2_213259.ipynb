{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "# Aula 2: Análise de Sentimentos usando Bag of Words\n",
        "\n",
        "Neste notebook iremos treinar um rede de uma única camada para fazer análise de sentimento usando o dataset IMDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vAaCL0u-zg3",
        "outputId": "8d715ef1-8614-44b1-c5b2-06bf6d3b05aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur Baia\n"
          ]
        }
      ],
      "source": [
        "nome = 'Arthur Baia'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhv5U8Muiyz_"
      },
      "source": [
        "# Importando as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzTCVXoOiyIs"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "# Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "2e8b0b65-d70d-4f3c-b985-b6479647138a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-31 12:54:03--  http://files.fast.ai/data/examples/imdb_sample.tgz\n",
            "Resolving files.fast.ai (files.fast.ai)... 104.26.2.19, 104.26.3.19, 172.67.69.159, ...\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.fast.ai/data/examples/imdb_sample.tgz [following]\n",
            "--2022-08-31 12:54:03--  https://files.fast.ai/data/examples/imdb_sample.tgz\n",
            "Connecting to files.fast.ai (files.fast.ai)|104.26.2.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571827 (558K) [application/x-gtar-compressed]\n",
            "Saving to: ‘imdb_sample.tgz’\n",
            "\n",
            "imdb_sample.tgz     100%[===================>] 558.42K   628KB/s    in 0.9s    \n",
            "\n",
            "2022-08-31 12:54:04 (628 KB/s) - ‘imdb_sample.tgz’ saved [571827/571827]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/examples/imdb_sample.tgz\n",
        "!tar -xzf imdb_sample.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "Carregamos o dataset .csv usando o pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "9f10f136-430b-4cd0-bb54-d15b2b84def1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4a1d1667-0a12-4d3d-abc6-f54a13dcbba8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a1d1667-0a12-4d3d-abc6-f54a13dcbba8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a1d1667-0a12-4d3d-abc6-f54a13dcbba8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a1d1667-0a12-4d3d-abc6-f54a13dcbba8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('imdb_sample/texts.csv')\n",
        "df.shape\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8dfjdJ-AV79"
      },
      "source": [
        "Iremos agora apenas selecionar 100 exemplos de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCoftmPmAfXE",
        "outputId": "24d4871d-ce44-4cc4-c6fa-24f4eedcd488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "treino.shape original: (800, 3)\n",
            "treino.shape depois: (100, 3)\n"
          ]
        }
      ],
      "source": [
        "treino = df[df['is_valid'] == False]  # Apenas treinamento, isto é, descartamos o dataset de validação.\n",
        "\n",
        "print('treino.shape original:', treino.shape)\n",
        "\n",
        "treino = treino[:100]  # Aqui truncamos o dataset para os 100 primeiros exemplos. \n",
        "\n",
        "print('treino.shape depois:', treino.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHus6FH7DftH"
      },
      "source": [
        "Iremos dividir este conjunto em entrada (X) e saída desejada (Y, target) e converter as strings \"positive\" e \"negative\" do target para valores booleanos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46RdLFLkEW-X",
        "outputId": "58483deb-8d99-4d9a-9f06-a0c74e9cfc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primeiras linhas de X_treino:\n",
            "0    Un-bleeping-believable! Meg Ryan doesn't even ...\n",
            "1    This is a extremely well-made film. The acting...\n",
            "2    Every once in a long while a movie will come a...\n",
            "3    Name just says it all. I watched this movie wi...\n",
            "4    This movie succeeds at being one of the most u...\n",
            "Name: text, dtype: object\n",
            "\n",
            "Primeiras linhas de Y_treino:\n",
            "0    negative\n",
            "1    positive\n",
            "2    negative\n",
            "3    positive\n",
            "4    negative\n",
            "Name: label, dtype: object\n",
            "\n",
            "Tamanho de Y_treino: torch.Size([100])\n",
            "5 primeiras linhas de Y_treino: tensor([0, 1, 0, 1, 0])\n",
            "Número de exemplos positivos: 51\n",
            "Número de exemplos negativos: 49\n"
          ]
        }
      ],
      "source": [
        "X_treino = treino['text']\n",
        "Y_treino = treino['label']\n",
        "\n",
        "print(f'Primeiras linhas de X_treino:\\n{X_treino.head()}\\n')\n",
        "print(f'Primeiras linhas de Y_treino:\\n{Y_treino.head()}\\n')\n",
        "\n",
        "mapeamento = {'positive': True, 'negative': False}\n",
        "Y_treino = Y_treino.map(mapeamento)\n",
        "Y_treino = torch.tensor(Y_treino.values, dtype=torch.long)\n",
        "print(f'Tamanho de Y_treino: {Y_treino.shape}')\n",
        "print(f'5 primeiras linhas de Y_treino: {Y_treino[:5]}')\n",
        "print(f'Número de exemplos positivos: {(Y_treino == True).sum()}')\n",
        "print(f'Número de exemplos negativos: {(Y_treino == False).sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlaPgP0Z_D4"
      },
      "source": [
        "# Definindo o tokenizador\n",
        "\n",
        "Agora temos a função de tokenização, isto é, que converte strings para tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIpp1C_qZ-QX"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str):\n",
        "    \"\"\"\n",
        "    Convert string to a list of tokens (i.e., words).\n",
        "    This function lower cases everything and removes punctuation.\n",
        "    \"\"\"\n",
        "    # Escreva aqui seu código.\n",
        "    return re.findall(r\"[\\w']+\", text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE7kwbIYlkPn"
      },
      "source": [
        "## Testando a função com um exemplo simples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS6QbpUwifyY",
        "outputId": "74651b57-3096-4518-f70f-bc063e5aa0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passou no assert!\n"
          ]
        }
      ],
      "source": [
        "assert tokenize(\"I like to eat pizza.\") == ['i', 'like', 'to', 'eat', 'pizza'], \"Não passou no assert.\"\n",
        "print('Passou no assert!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbf_yO6XJ_hY"
      },
      "source": [
        "# Definindo o vocabulário\n",
        "\n",
        "Selecionaremos os `max_tokens` (ex: 1000) tokens mais frequentes do dataset de treino como sendo nosso vocabulário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqm_aWuYuaGi"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVc8uucLK4pP"
      },
      "outputs": [],
      "source": [
        "def create_vocab(texts: List[str], max_tokens: int):\n",
        "    \"\"\"\n",
        "    Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
        "    \"\"\"\n",
        "    # Escreva aqui seu código.\n",
        "    L = [word for phrase in list(map(tokenize, texts)) for word in phrase]\n",
        "    k = max_tokens\n",
        "    vocab = lambda L, k : {value: key for key, value in enumerate(dict(Counter(L).most_common(k)))}\n",
        "    return vocab(L, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMYyPXpYlr64"
      },
      "source": [
        "## Testando a função\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF_9u5e4FdD5"
      },
      "outputs": [],
      "source": [
        "# Escreva aqui seu(s) assert(s).\n",
        "teste = ['Fui na padaria e voltei com um pão. Depois fui ao bandejão.']\n",
        "max_tokens_test = len(teste[0])\n",
        "assert create_vocab(teste, max_tokens_test) == {'fui': 0, 'na': 1, 'padaria': 2, 'e': 3, 'voltei': 4, 'com': 5, 'um': 6, 'pão': 7, 'depois': 8, 'ao': 9, 'bandejão': 10} \n",
        "max_tokens = 1000\n",
        "vocab = create_vocab(treino['text'],max_tokens)\n",
        "assert len(vocab) == max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wki1S4LMKCrh"
      },
      "source": [
        "# Função para converter string para Bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TdUgA5RMl_4"
      },
      "outputs": [],
      "source": [
        "def convert_to_bow(text: str, vocab):\n",
        "    \"\"\"\n",
        "    Returns a bag-of-word vector of size len(vocab).\n",
        "    \"\"\"\n",
        "    # Escreva aqui seu código.\n",
        "    tokenized = [vocab[token] if token in vocab else -1 for token in tokenize(text)]\n",
        "    bow = [1 if index in tokenized else 0 for index in torch.arange(len(vocab))]\n",
        "    return torch.tensor(bow, ).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLnkDcWil-Jd"
      },
      "source": [
        "## Testando a função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTu0EBGVGw_8",
        "outputId": "7781ac0c-8414-4b5d-b728-aa4f4fab4a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "passou no assert\n"
          ]
        }
      ],
      "source": [
        "# Escreva aqui seu(s) assert(s).\n",
        "vocab = create_vocab(treino['text'],max_tokens)\n",
        "assert len(convert_to_bow('the movie is pretty pretty pretty good, it shows the history of vasco da gama!', vocab)) == max_tokens #Cehca se gerou tensor com o tamanho certo\n",
        "assert torch.equal(convert_to_bow('jeiendkv jeiwhrghij erickmrncje ekcmfmrkjc kjenejxcnenc', vocab), convert_to_bow('jeiwhrghij jeiendkv ekcmfmrkjc erickmrncje kjenejxcnenc', vocab)) #ordem diferente produz mesmo vetor, então one hot funciona\n",
        "print('passou no assert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_6pddDHEM_r"
      },
      "source": [
        "## Definindo a Rede Neural\n",
        "\n",
        "**Entrada:**\n",
        "\n",
        "$x \\in R^{B \\times |V|}$     (bag-of-words)\n",
        "\n",
        "**Parametros:**\n",
        "\n",
        "$W \\in R^{|V| \\times K}$    (weights: matriz de pesos)\n",
        "\n",
        "$b \\in R^{K}$    (bias/viés)\n",
        "\n",
        "**Saída:**\n",
        "\n",
        "$p \\in R^{B \\times K}$  (probabilidade de cada classe)\n",
        "\n",
        "\n",
        "**Onde:**\n",
        "\n",
        "$K$ = número de classes\n",
        "\n",
        "$B$ = tamanho do batch\n",
        "\n",
        "$|V|$ = tamanho do vocabulário\n",
        "\n",
        "**Definição da rede:**\n",
        "\n",
        "$z = xW + b$   (camada linear. $z$ é chamado de logits)\n",
        "\n",
        "$p_i = \\frac{e^{z_i}}{\\sum_{j=0}^{K-1} e^{z_j}}$   (softmax)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLReRSuDEPLL"
      },
      "outputs": [],
      "source": [
        "class MyModel():\n",
        "\n",
        "    def __init__(self, dim: int):\n",
        "        # Escreva seu código aqui.\n",
        "        self.k = 2 # number of classes\n",
        "        self.weights = torch.randn((dim,k))*0.001 - 0.0005 #1000x2\n",
        "        self.weights.requires_grad = True\n",
        "        self.bias = torch.zeros(k) #2x1\n",
        "        self.bias.requires_grad = True\n",
        "\n",
        "\n",
        "    def __call__(self, x): # x = \n",
        "        # Escreva seu código aqui.\n",
        "        z =  torch.matmul(x, self.weights) + self.bias # multiplicação X(B,max_tokens)*W(max_tokens, 2) = Z(B,2) + Biases(2,1)\n",
        "        max_row = torch.max(z, dim = 1, keepdim=True)\n",
        "        ez = torch.exp(z - max_row.values)\n",
        "        softmax = ez / ez.sum(dim = 1, keepdim=True)\n",
        "        return softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8eWRvN79f4O"
      },
      "source": [
        "## Testando modelo com uma entrada aleatória\n",
        "\n",
        "Escreva abaixo um pequeno código para testar se seu modelo processa uma matriz de entrada de tamanho `batch_size, dim`, ou seja, a matriz contém `batch_size` exemplos, cada um sendo representado por um vetor de tamanho `dim`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-lupvM_HxOJ"
      },
      "outputs": [],
      "source": [
        "# Escreva seu código aqui\n",
        "model = MyModel(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ21oxlP9SSB",
        "outputId": "58209fca-3acc-461a-dae3-8fd83ba0db46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passou nos asserts.\n"
          ]
        }
      ],
      "source": [
        "vocab = create_vocab(treino['text'],max_tokens)\n",
        "X = torch.stack([convert_to_bow('the movie is pretty pretty pretty good, it shows the history of vasco da gama!', vocab), \n",
        "                 convert_to_bow('the movie is pretty pretty pretty good, curb your enthusiam when watching it!',  vocab), \n",
        "                 convert_to_bow('the movie is pretty good, dev patel nails it!', vocab)])\n",
        "model_tested = model(X)\n",
        "batch_size = len(X)\n",
        "assert model_tested.shape == (batch_size, model.k) # Checa se a matriz final tem o shape desejado\n",
        "assert len(model_tested) == batch_size, \"Não passou no assert.\"\n",
        "print('Passou nos asserts.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EXAYdOvGcF"
      },
      "source": [
        "# Função de custo Entropia Cruzada\n",
        "\n",
        "$y \\in R^{K}$  (target),\n",
        "\n",
        "a equação da entropia cruzada associada a um exemplo é dada por:\n",
        "\n",
        "$L = \\sum_{i=0}^{K-1} -y_i \\log p_i$   (esta é a loss por exemplo)\n",
        "\n",
        "Se $y$ for um vetor one-hot (apenas um dos elementos é diferente de zero), podemos simplicar a equação acima para:\n",
        "\n",
        "$L = -\\log p_i$\n",
        "\n",
        "Onde $i$ é o indice da classe correta. Ou seja, $p_i$ é a probabilidade que o modelo colocou na classe correta.\n",
        "\n",
        "A função de custo é a **média** da entropia cruzada de cada exemplo no batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI2Ktf1CvJwV"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(probs, targets):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      probs: a float32 matrix of shape (batch_size, number of classes)\n",
        "      targets: a long (int64) array of shape (batch_size)\n",
        "\n",
        "    Returns:\n",
        "      Mean loss in the batch.\n",
        "    \"\"\"\n",
        "    # Rescreva o código abaixo sem usar laço.\n",
        "    # batch_size = probs.shape[0]\n",
        "    # losses = []\n",
        "    # for i in range(batch_size):\n",
        "    #   print(targets[i],probs[i, targets[i]], -torch.log(probs[i, targets[i]]))\n",
        "    #   losses.append(-torch.log(probs[i, targets[i]]))\n",
        "    \n",
        "    # losses = torch.stack(losses)\n",
        "    return (-torch.log(probs[torch.arange(len(targets)), targets])).mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgECDC5uqRo"
      },
      "source": [
        "## Testando a função entropia cruzada com probabilidades de 50%\n",
        "\n",
        "Escreva abaixo um pequeno código para testar se a entropia cruzada confere com a resposta do problema 3.6 do exercício da semana passada. Crie um tensor para as probabilidades (50%) e um target também aleatório balanceado e calcule a cross entropia. Qual é o valor esperado da cross entropia nesse caso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-10K5Jecve--",
        "outputId": "45477c61-9bba-49c9-c97a-d127268c26d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passou no Assert\n"
          ]
        }
      ],
      "source": [
        "# escreva seu código aqui\n",
        "\n",
        "k = 2\n",
        "batch_size = 1000\n",
        "probs = torch.ones(batch_size, k)*.5\n",
        "targets = torch.randint(0, k, (batch_size,))\n",
        "L = cross_entropy_loss(probs, targets)\n",
        "assert torch.allclose(torch.log(torch.tensor(k)).data,L) # Garante que o valor esperado é igual ao valor calculado\n",
        "print('Passou no Assert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcTeNuJF1Mp"
      },
      "source": [
        "# Convertendo dataset de treino para uma matriz de bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGIFOJZcFs7b",
        "outputId": "8f392a82-4d12-400d-bba0-db1e51c05626"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 1.],\n",
            "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "vocab = create_vocab(X_treino, max_tokens=1000)\n",
        "bows = [convert_to_bow(text, vocab) for text in X_treino]\n",
        "X = torch.stack(bows)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSdBQozHlT59"
      },
      "source": [
        "# Laço de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j3a3FsBlWCy",
        "outputId": "0c8915ec-fdc7-44d5-9325-6b94df3e542f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iteration: 0  loss: 0.691838  exp(loss): 1.9974\n",
            "iteration: 1  loss: 0.650770  exp(loss): 1.9170\n",
            "iteration: 2  loss: 0.614112  exp(loss): 1.8480\n",
            "iteration: 3  loss: 0.581033  exp(loss): 1.7879\n",
            "iteration: 4  loss: 0.551238  exp(loss): 1.7354\n",
            "iteration: 5  loss: 0.524212  exp(loss): 1.6891\n",
            "iteration: 6  loss: 0.499635  exp(loss): 1.6481\n",
            "iteration: 7  loss: 0.477197  exp(loss): 1.6116\n",
            "iteration: 8  loss: 0.456642  exp(loss): 1.5788\n",
            "iteration: 9  loss: 0.437748  exp(loss): 1.5492\n",
            "iteration: 10  loss: 0.420324  exp(loss): 1.5225\n",
            "iteration: 11  loss: 0.404207  exp(loss): 1.4981\n",
            "iteration: 12  loss: 0.389257  exp(loss): 1.4759\n",
            "iteration: 13  loss: 0.375350  exp(loss): 1.4555\n",
            "iteration: 14  loss: 0.362382  exp(loss): 1.4367\n",
            "iteration: 15  loss: 0.350259  exp(loss): 1.4194\n",
            "iteration: 16  loss: 0.338901  exp(loss): 1.4034\n",
            "iteration: 17  loss: 0.328238  exp(loss): 1.3885\n",
            "iteration: 18  loss: 0.318207  exp(loss): 1.3747\n",
            "iteration: 19  loss: 0.308753  exp(loss): 1.3617\n",
            "iteration: 20  loss: 0.299827  exp(loss): 1.3496\n",
            "iteration: 21  loss: 0.291387  exp(loss): 1.3383\n",
            "iteration: 22  loss: 0.283392  exp(loss): 1.3276\n",
            "iteration: 23  loss: 0.275809  exp(loss): 1.3176\n",
            "iteration: 24  loss: 0.268607  exp(loss): 1.3081\n",
            "iteration: 25  loss: 0.261757  exp(loss): 1.2992\n",
            "iteration: 26  loss: 0.255234  exp(loss): 1.2908\n",
            "iteration: 27  loss: 0.249015  exp(loss): 1.2828\n",
            "iteration: 28  loss: 0.243079  exp(loss): 1.2752\n",
            "iteration: 29  loss: 0.237408  exp(loss): 1.2680\n",
            "iteration: 30  loss: 0.231984  exp(loss): 1.2611\n",
            "iteration: 31  loss: 0.226791  exp(loss): 1.2546\n",
            "iteration: 32  loss: 0.221815  exp(loss): 1.2483\n",
            "iteration: 33  loss: 0.217043  exp(loss): 1.2424\n",
            "iteration: 34  loss: 0.212462  exp(loss): 1.2367\n",
            "iteration: 35  loss: 0.208062  exp(loss): 1.2313\n",
            "iteration: 36  loss: 0.203832  exp(loss): 1.2261\n",
            "iteration: 37  loss: 0.199762  exp(loss): 1.2211\n",
            "iteration: 38  loss: 0.195843  exp(loss): 1.2163\n",
            "iteration: 39  loss: 0.192068  exp(loss): 1.2118\n",
            "iteration: 40  loss: 0.188428  exp(loss): 1.2074\n",
            "iteration: 41  loss: 0.184917  exp(loss): 1.2031\n",
            "iteration: 42  loss: 0.181528  exp(loss): 1.1990\n",
            "iteration: 43  loss: 0.178254  exp(loss): 1.1951\n",
            "iteration: 44  loss: 0.175091  exp(loss): 1.1914\n",
            "iteration: 45  loss: 0.172032  exp(loss): 1.1877\n",
            "iteration: 46  loss: 0.169073  exp(loss): 1.1842\n",
            "iteration: 47  loss: 0.166209  exp(loss): 1.1808\n",
            "iteration: 48  loss: 0.163436  exp(loss): 1.1775\n",
            "iteration: 49  loss: 0.160748  exp(loss): 1.1744\n",
            "iteration: 50  loss: 0.158144  exp(loss): 1.1713\n",
            "iteration: 51  loss: 0.155618  exp(loss): 1.1684\n",
            "iteration: 52  loss: 0.153167  exp(loss): 1.1655\n",
            "iteration: 53  loss: 0.150789  exp(loss): 1.1628\n",
            "iteration: 54  loss: 0.148479  exp(loss): 1.1601\n",
            "iteration: 55  loss: 0.146236  exp(loss): 1.1575\n",
            "iteration: 56  loss: 0.144056  exp(loss): 1.1549\n",
            "iteration: 57  loss: 0.141937  exp(loss): 1.1525\n",
            "iteration: 58  loss: 0.139876  exp(loss): 1.1501\n",
            "iteration: 59  loss: 0.137871  exp(loss): 1.1478\n",
            "iteration: 60  loss: 0.135920  exp(loss): 1.1456\n",
            "iteration: 61  loss: 0.134021  exp(loss): 1.1434\n",
            "iteration: 62  loss: 0.132171  exp(loss): 1.1413\n",
            "iteration: 63  loss: 0.130369  exp(loss): 1.1392\n",
            "iteration: 64  loss: 0.128614  exp(loss): 1.1373\n",
            "iteration: 65  loss: 0.126902  exp(loss): 1.1353\n",
            "iteration: 66  loss: 0.125234  exp(loss): 1.1334\n",
            "iteration: 67  loss: 0.123606  exp(loss): 1.1316\n",
            "iteration: 68  loss: 0.122019  exp(loss): 1.1298\n",
            "iteration: 69  loss: 0.120469  exp(loss): 1.1280\n",
            "iteration: 70  loss: 0.118957  exp(loss): 1.1263\n",
            "iteration: 71  loss: 0.117480  exp(loss): 1.1247\n",
            "iteration: 72  loss: 0.116038  exp(loss): 1.1230\n",
            "iteration: 73  loss: 0.114629  exp(loss): 1.1215\n",
            "iteration: 74  loss: 0.113253  exp(loss): 1.1199\n",
            "iteration: 75  loss: 0.111907  exp(loss): 1.1184\n",
            "iteration: 76  loss: 0.110592  exp(loss): 1.1169\n",
            "iteration: 77  loss: 0.109306  exp(loss): 1.1155\n",
            "iteration: 78  loss: 0.108048  exp(loss): 1.1141\n",
            "iteration: 79  loss: 0.106817  exp(loss): 1.1127\n",
            "iteration: 80  loss: 0.105613  exp(loss): 1.1114\n",
            "iteration: 81  loss: 0.104434  exp(loss): 1.1101\n",
            "iteration: 82  loss: 0.103280  exp(loss): 1.1088\n",
            "iteration: 83  loss: 0.102151  exp(loss): 1.1076\n",
            "iteration: 84  loss: 0.101045  exp(loss): 1.1063\n",
            "iteration: 85  loss: 0.099961  exp(loss): 1.1051\n",
            "iteration: 86  loss: 0.098899  exp(loss): 1.1040\n",
            "iteration: 87  loss: 0.097859  exp(loss): 1.1028\n",
            "iteration: 88  loss: 0.096839  exp(loss): 1.1017\n",
            "iteration: 89  loss: 0.095840  exp(loss): 1.1006\n",
            "iteration: 90  loss: 0.094860  exp(loss): 1.0995\n",
            "iteration: 91  loss: 0.093899  exp(loss): 1.0984\n",
            "iteration: 92  loss: 0.092957  exp(loss): 1.0974\n",
            "iteration: 93  loss: 0.092032  exp(loss): 1.0964\n",
            "iteration: 94  loss: 0.091125  exp(loss): 1.0954\n",
            "iteration: 95  loss: 0.090235  exp(loss): 1.0944\n",
            "iteration: 96  loss: 0.089361  exp(loss): 1.0935\n",
            "iteration: 97  loss: 0.088504  exp(loss): 1.0925\n",
            "iteration: 98  loss: 0.087662  exp(loss): 1.0916\n",
            "iteration: 99  loss: 0.086835  exp(loss): 1.0907\n"
          ]
        }
      ],
      "source": [
        "num_iterations = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "model = MyModel(dim=len(vocab))\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    # Zera os gradientes\n",
        "    if model.weights.grad is not None:\n",
        "        model.weights.grad.data.zero_()\n",
        "        model.bias.grad.data.zero_()\n",
        "\n",
        "    probs = model(X)\n",
        "    loss = cross_entropy_loss(probs, Y_treino)\n",
        "    print(f'iteration: {i}  loss: {loss:.6f}  exp(loss): {torch.exp(loss):.4f}')\n",
        "    loss.backward()\n",
        "\n",
        "    #Atualiza os pesos\n",
        "    model.weights.data -= learning_rate * model.weights.grad.data\n",
        "    model.bias.data -= model.bias.data - learning_rate * model.bias.grad.data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
