{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "dd195f96-0d60-400c-c45f-b036e115b8c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur\n"
          ]
        }
      ],
      "source": [
        "nome = \"Arthur\"\n",
        "print(f\"Meu nome é {nome}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção (versão eficiente)\n",
        "\n",
        "Este exercício é similar ao da aula 5, mas iremos agora treinar *eficientemente* uma rede neural com uma ou mais camadas de auto-atenção para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Para tanto, deve-se implementar:\n",
        "1. A máscara causal de atenção. Ela possibilitará que, durante o treinamento, com apenas uma forward+backward pass na rede, tenhamos as losses para todos os tokens de entrada (slide 117).\n",
        "2. A máscara de PADs, que permite que usemos sequencias de comprimento variável no mesmo batch (slide 118).\n",
        "3. Múltiplas cabeças."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from typing import List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w9f3PfifAwpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 17 19:23:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   55C    P5    10W /  N/A |    725MiB /  5944MiB |     23%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1958      G   /usr/lib/xorg/Xorg                 77MiB |\n",
            "|    0   N/A  N/A      3689      G   /usr/lib/xorg/Xorg                196MiB |\n",
            "|    0   N/A  N/A      3865      G   /usr/bin/gnome-shell              105MiB |\n",
            "|    0   N/A  N/A      4339      G   /usr/lib/firefox/firefox          152MiB |\n",
            "|    0   N/A  N/A      7920      G   ...895815506602292515,131072       33MiB |\n",
            "|    0   N/A  N/A     15925      G   ...RendererForSitePerProcess      143MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "whTCe2i7AtoV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda:0\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print(\"Using {}\".format(device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2wbnfzst5O3k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0HIN_xLI_TuT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "Terrfic film with a slightyly slow start - give it a chance to start cooking. Story builds in intere\n",
            "my friend bought the movie for 5 (its is not even 1 cent worth), because they wrote it was like Ame\n",
            "I haven't laughed this hard at a movie in a long time. I got to go to an advance screening, and was \n",
            "3 últimas amostras treino:\n",
            "I saw this film last night following a lot of good reviews from many sources. I would like to point \n",
            "Parsifal (1982) Starring Michael Kutter, Armin Jordan, Robert Lloyd, Martin Sperr, Edith Clever, Aag\n",
            "This is a bit of a puzzle for a lot of the artsy Lynch crowd. They tend to try to write this off as \n",
            "3 primeiras amostras validação:\n",
            "Robot Jox tries hard, but is fundamentally a series of fight scenes strung together -- robot against\n",
            "On Steve Irwin's show, he's hillarious. He doesn't even try to be funny and he just is but his movie\n",
            "I didn't expect a movie as good as \"In The Line of Fire\" or an episode of \"24\", but it looked like t\n",
            "3 últimas amostras validação:\n",
            "I am a huge fan of Vonnegut's work and I'm very fond of this movie, but I wouldn't say that this is \n",
            "The premise for this movie is simple and so is the script: an elderly Muslim gets his teenage son to\n",
            "Popular radio storyteller Gabriel No one(Robin Williams,scraggy and speaking in hushed,hypnotic tone\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "\n",
        "x_train_pos = load_texts(\"aclImdb/train/pos\")\n",
        "x_train_neg = load_texts(\"aclImdb/train/neg\")\n",
        "x_test_pos = load_texts(\"aclImdb/test/pos\")\n",
        "x_test_neg = load_texts(\"aclImdb/test/neg\")\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), \"amostras de treino.\")\n",
        "print(len(x_valid), \"amostras de desenvolvimento.\")\n",
        "print(len(x_test), \"amostras de teste.\")\n",
        "\n",
        "print(\"3 primeiras amostras treino:\")\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print(\"3 últimas amostras treino:\")\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print(\"3 primeiras amostras validação:\")\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print(\"3 últimas amostras validação:\")\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arthur/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, corpus: List[str], tokenizer, max_seq_length):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        data = []\n",
        "\n",
        "        self.tokenized = self.tokenizer.batch_encode_plus(\n",
        "            [x.replace(\"<br />\", \" \") for x in corpus],\n",
        "            padding=False,\n",
        "            truncation=False,\n",
        "            return_tensors=None,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "\n",
        "        for tokens, attention_masks in zip(\n",
        "            self.tokenized[\"input_ids\"], self.tokenized[\"attention_mask\"]\n",
        "        ):\n",
        "            print(tokens, attention_masks, len(tokens), len(attention_masks))\n",
        "            assert len(tokens) == len(attention_masks)\n",
        "            data.extend(\n",
        "                [\n",
        "                    [\n",
        "                        tokens[i : i + max_seq_length + 1],\n",
        "                        attention_masks[i : i + max_seq_length + 1],\n",
        "                    ]\n",
        "                    for i in range(0, len(tokens) - max_seq_length, max_seq_length)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        self.data = torch.IntTensor(data)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            self.data[index][0, :-1],\n",
        "            self.data[index][1, :-1],\n",
        "            self.data[index][0, 1:].long(),\n",
        "            self.data[index][1, 1:],\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, emb_dim, heads) -> None:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.Wq = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.Wk = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.Wv = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "        self.Wo = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "\n",
        "    def forward(self, input_embeddings, mask=None):\n",
        "        Q = self.Wq(input_embeddings)\n",
        "        K = self.Wk(input_embeddings)\n",
        "        V = self.Wv(input_embeddings)\n",
        "\n",
        "        Q = Q.reshape(Q.shape[0], Q.shape[1], self.heads, -1)\n",
        "        K = K.reshape(K.shape[0], K.shape[1], self.heads, -1)\n",
        "        V = V.reshape(V.shape[0], V.shape[1], self.heads, -1)\n",
        "\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "\n",
        "        scores = Q @ torch.transpose(K, -2, -1) / math.sqrt(Q.shape[-1])\n",
        "\n",
        "        if mask is not None:\n",
        "            scores[mask == 0] = -1e8\n",
        "\n",
        "        attention_weigths = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        E = attention_weigths @ V\n",
        "\n",
        "        E = E.transpose(1, 2)\n",
        "\n",
        "        E = E.reshape(E.shape[0], E.shape[1], -1)\n",
        "\n",
        "        E = self.out_proj(E)\n",
        "\n",
        "        return E\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyAttentionModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_seq_length: int,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int = 50,\n",
        "        heads: int = 5,\n",
        "        eos_token_id: int = None,\n",
        "    ):\n",
        "        super(MyAttentionModel, self).__init__()\n",
        "\n",
        "        self.generating = False\n",
        "\n",
        "        self.eos_token_id = eos_token_id\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.heads = heads\n",
        "\n",
        "        self.causal_mask = nn.Parameter(\n",
        "            data=torch.tril(torch.ones(max_seq_length, max_seq_length)),\n",
        "            requires_grad=False,\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.15)\n",
        "\n",
        "        self.tokens_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_embeddings = nn.Parameter(\n",
        "            data=torch.normal(0, 0.1, size=(max_seq_length, embedding_dim))\n",
        "        )\n",
        "\n",
        "        self.self_attention = MultiHeadSelfAttention(\n",
        "            heads=heads, embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "        self.feed_foward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "        )\n",
        "\n",
        "        self.language_head = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        input_embeddings = (\n",
        "            self.tokens_embeddings(x) + self.positional_embeddings[:seq_len, :]\n",
        "        )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.reshape(batch_size, 1, 1, seq_len).expand(\n",
        "                -1, self.heads, seq_len, -1\n",
        "            )\n",
        "            causal_mask = (\n",
        "                self.causal_mask[:seq_len, :seq_len]\n",
        "                .reshape(1, 1, seq_len, seq_len)\n",
        "                .expand(batch_size, self.heads, seq_len, seq_len)\n",
        "            )\n",
        "            mask = attention_mask * causal_mask\n",
        "        else:\n",
        "            mask = (\n",
        "                self.causal_mask[:seq_len, :seq_len]\n",
        "                .reshape(1, 1, seq_len, seq_len)\n",
        "                .expand(batch_size, self.heads, seq_len, seq_len)\n",
        "            )\n",
        "\n",
        "        E = self.self_attention(input_embeddings, mask=mask)\n",
        "        E = E + self.dropout(input_embeddings)  # skip-connection\n",
        "\n",
        "        y = self.feed_foward(E)\n",
        "        y = y + self.dropout(E)  # skip-connection\n",
        "\n",
        "        if self.generating:\n",
        "            logits = self.language_head(y[:, -1, :])\n",
        "        else:\n",
        "            logits = self.language_head(y)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
