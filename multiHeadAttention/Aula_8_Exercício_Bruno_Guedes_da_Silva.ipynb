{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem com auto-atenção (versão eficiente)\n",
        "\n",
        "Este exercício é similar ao da aula 5, mas iremos agora treinar *eficientemente* uma rede neural com uma ou mais camadas de auto-atenção para prever a próxima palavra de um texto, data as palavras anteriores como entrada. \n",
        "\n",
        "Para tanto, deve-se implementar:\n",
        "1. A máscara causal de atenção. Ela possibilitará que, durante o treinamento, com apenas uma forward+backward pass na rede, tenhamos as losses para todos os tokens de entrada (slide 117).\n",
        "2. A máscara de PADs, que permite que usemos sequencias de comprimento variável no mesmo batch (slide 118).\n",
        "3. Múltiplas cabeças."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List, Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9f3PfifAwpU",
        "outputId": "1721f5f3-b5d9-40e4-dc5c-6a17716ee728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct 17 19:23:31 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   53C    P8     7W /  N/A |    690MiB /  5944MiB |      4%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1958      G   /usr/lib/xorg/Xorg                 77MiB |\n",
            "|    0   N/A  N/A      3689      G   /usr/lib/xorg/Xorg                186MiB |\n",
            "|    0   N/A  N/A      3865      G   /usr/bin/gnome-shell              105MiB |\n",
            "|    0   N/A  N/A      4339      G   /usr/lib/firefox/firefox          152MiB |\n",
            "|    0   N/A  N/A      7920      G   ...895815506602292515,131072       33MiB |\n",
            "|    0   N/A  N/A     15925      G   ...RendererForSitePerProcess      118MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTCe2i7AtoV",
        "outputId": "b897cea3-b96b-490d-985f-910ea4b49e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# 1) Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "0ead0894-75f3-4294-e949-66a8fe354ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "b8ad2cc7-8389-49c5-aabf-ebc07e443310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "Whoever made this movie must have done it as a joke. I mean, this was the stupidest movie I think I \n",
            "This movie has beautiful scenery. Unfortunately it has no plot. In order to have a plot there must b\n",
            "<br /><br />I have to admit to enjoying bad movies. I love them I watch all of them. Horror especial\n",
            "3 últimas amostras treino:\n",
            "There is no greater disservice to do to history than to misrepresent it. This takes the easiest and \n",
            "I watched this movie and the original Carlitos Way back to back. The difference between the two is d\n",
            "I didn't mind all the walking. People really did walk places back then. It loaned an air of authenti\n",
            "3 primeiras amostras validação:\n",
            "I went to see this movie twice within a week and can only sum it up in one word (which I normally do\n",
            "I really did like this show, once upon a time. That is, until I realized all the faults in it. It's \n",
            "I recently found this movie on VHS after looking for it for a number of years, I was not disappointe\n",
            "3 últimas amostras validação:\n",
            "A friend told me of John Fante last summer after we got into a conversation about Charles Bukowski. \n",
            "This is one of those westerns that, well, stands practically alone in the unrelieved quality of its \n",
            "I just picked up the DVD release of this movie while on holiday in Norway where it has been released\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOteRsLWjcfW"
      },
      "source": [
        "# 2) Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J4k-v7eNjeff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:16: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "<>:16: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "/tmp/ipykernel_45246/226890427.py:16: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  if len(tokens) % len_multiple is not 0:\n"
          ]
        }
      ],
      "source": [
        "class IMDBTokenizer():\n",
        "\n",
        "  def __init__(self, max_tokens: int = 1000):\n",
        "    self.max_tokens = max_tokens\n",
        "\n",
        "  def __call__(self, text: str, padding: bool = False, truncation: bool = False, max_length: int = 50, len_multiple: int = None):\n",
        "    tokens = self.encode(text)\n",
        "    attention_mask = [1] * len(tokens)\n",
        "\n",
        "    if truncation and len(tokens) > max_length:\n",
        "      tokens = tokens[:max_length]\n",
        "      attention_mask = attention_mask[:max_length]\n",
        "    if padding:\n",
        "      missing_size = 0\n",
        "      if len_multiple is not None:\n",
        "        if len(tokens) % len_multiple is not 0:\n",
        "          missing_size = len_multiple - len(tokens) % len_multiple + 1\n",
        "        else:\n",
        "          missing_size = 1\n",
        "      else :\n",
        "        if len(tokens) < max_length:\n",
        "          missing_size = max_length - len(tokens)\n",
        "\n",
        "      tokens.extend([self.vocab['<pad>']] * missing_size)\n",
        "      attention_mask.extend([0] * missing_size)\n",
        "    \n",
        "    return tokens, attention_mask\n",
        "\n",
        "  def encode(self, text: str):\n",
        "    tokens = self.tokenize(text)\n",
        "    tokens = [self.vocab.get(t, self.vocab['<unk>']) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "  def decode(self, tokens: List[int]):\n",
        "    decoder_dict = dict(zip(self.vocab.values(), self.vocab.keys()))\n",
        "    texts = [decoder_dict[t] for t in tokens]\n",
        "    \n",
        "    def replace_fn(match):\n",
        "      return match.group(0).replace(' ', '')\n",
        "\n",
        "    return re.sub(r'\\s[,.!?\\']', replace_fn, ' '.join(texts))\n",
        "\n",
        "  def create_vocab(self, corpus: List[str]):\n",
        "    texts = [self.tokenize(t) for t in corpus]\n",
        "    tokens = [t for tokens in texts for t in tokens]\n",
        "    vocab = collections.Counter(tokens).most_common(self.max_tokens - 4)\n",
        "    self.vocab = {v[0]:i+4 for i, v in enumerate(vocab)}\n",
        "    self.vocab['<sos>'] = 0\n",
        "    self.vocab['<eos>'] = 1\n",
        "    self.vocab['<pad>'] = 2\n",
        "    self.vocab['<unk>'] = 3\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    self.eos_token_id = self.vocab['<eos>']\n",
        "    self.pad_token_id = self.vocab['<pad>']\n",
        "    self.unk_token_id = self.vocab['<unk>']\n",
        "\n",
        "  def tokenize(self, text: str):\n",
        "    \n",
        "    #-----Optionally removes html for breaking line------------\n",
        "    # Obs: Essa troca causa um aumento no erro do modelo.\n",
        "    # Muito provavelmente devido ao fato de que a sequência de <br /><br /> \n",
        "    #sempre aparece nesta ordem, gerando sempre a mesma sequência de 8 tokens:\n",
        "    # '<,' 'br', '\\', '>', '<', 'br', '\\', '>'\n",
        "    # Logo o modelo fica enviesado a sempre prever essa sequência quando parte\n",
        "    # dela aparece (e.g. prever \\ quando a sequencia de entrada termina em <br)\n",
        "    # o que acaba diminuindo seu erro\n",
        "    #text = re.sub(re.escape('°'), ' ', text) # The symbol ° only appears 4 times in the dataset and will be used to represent <br /><br />  (line breaks)\n",
        "    text = re.sub(re.escape('<br /><br />'), ' ', text)\n",
        "    #------------------------------------------------------\n",
        "\n",
        "    #Optionally ignore ' \" \\ | / and some others special characters\n",
        "    #tokens = re.findall(r'\\w+|[,.!?-]', text.lower())\n",
        "    \n",
        "    tokens = re.findall(r'<sos>|<eos>|\\w+|[^\\w\\s]', text.lower())\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZUgg53w8IBC"
      },
      "source": [
        "## 2.1) Testando tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4QphNdGmNPu",
        "outputId": "e9452873-b995-49e4-9baf-674e6f25fe30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'two': 4, 'four': 5, 'is': 6, 'plus': 7, '.': 8, 'eigth': 9, '!': 10, 'divided': 11, 'by': 12, '<sos>': 0, '<eos>': 1, '<pad>': 2, '<unk>': 3}\n"
          ]
        }
      ],
      "source": [
        "text = '<sos> Four divided by two is two. <eos>'\n",
        "text2 = 'Words not in vocab'\n",
        "corpus = ['Two plus two is four.', 'Four plus four is eigth!', 'Four divided by two is two.']\n",
        "\n",
        "tokenizer = IMDBTokenizer()\n",
        "tokenizer.create_vocab(corpus)\n",
        "print(tokenizer.vocab)\n",
        "assert tokenizer.encode(text) == [0, 5, 11, 12, 4, 6, 4, 8, 1]\n",
        "assert tokenizer.encode(text2) == [tokenizer.unk_token_id]*4\n",
        "\n",
        "assert tokenizer.decode([0, 5, 11, 12, 4, 6, 4, 8, 1]) == '<sos> four divided by two is two. <eos>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkeXmlj78Kbd"
      },
      "source": [
        "# 3) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9X6TKqJj9TSu"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "  def __init__(self, corpus: List[str], tokenizer: Type[IMDBTokenizer], max_seq_length: int = 5, shifting_window: bool = False):\n",
        "    \n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for text in corpus:\n",
        "      if shifting_window:\n",
        "        tokens, attention_masks = self.tokenizer('<sos> ' + text + ' <eos>')\n",
        "        data.extend([[tokens[i:i+max_seq_length+1], attention_masks[i:i+max_seq_length+1]] for i in range(len(tokens)-max_seq_length)])\n",
        "      else:\n",
        "        tokens, attention_masks = self.tokenizer('<sos> ' + text + ' <eos>', padding=True, len_multiple=max_seq_length)\n",
        "        data.extend([[tokens[i:i+max_seq_length+1], attention_masks[i:i+max_seq_length+1]] for i in range(0, len(tokens) - max_seq_length, max_seq_length)])\n",
        "\n",
        "    self.data = torch.IntTensor(data)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    target_ids = self.data[index][0, 1:].long()\n",
        "\n",
        "    ignore_loss = (target_ids == self.tokenizer.pad_token_id) + (target_ids == self.tokenizer.unk_token_id)\n",
        "    target_ids[ignore_loss] = -100\n",
        "\n",
        "    return self.data[index][0, :-1], self.data[index][1, :-1], target_ids, self.data[index][1, 1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = IMDBTokenizer(max_tokens=3)\n",
        "tokenizer.create_vocab(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0, 3, 3, 3, 3], dtype=torch.int32), tensor([1, 1, 1, 1, 1], dtype=torch.int32), tensor([-100, -100, -100, -100, -100]), tensor([1, 1, 1, 1, 1], dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "train_dataset = IMDBDataset(x_train[:1], tokenizer, max_seq_length=5, shifting_window=True)\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nrc30MDCLDU",
        "outputId": "fc74000f-9e2f-4812-edd1-01c27ae661a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([7, 4], dtype=torch.int32), tensor([1, 1], dtype=torch.int32), tensor([4, 6]), tensor([1, 1], dtype=torch.int32)) tensor([4, 7, 4, 6, 5, 8], dtype=torch.int32)\n",
            "(tensor([0, 3], dtype=torch.int32), tensor([1, 1], dtype=torch.int32), tensor([-100, -100]), tensor([1, 1], dtype=torch.int32)) tensor([3, 3, 3, 3, 3, 3], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "corpus = ['Two plus two is four.', 'Four plus four is eigth!', 'Four divided by two is two.']\n",
        "\n",
        "tokenizer = IMDBTokenizer()\n",
        "tokenizer.create_vocab(corpus)\n",
        "\n",
        "dataset = IMDBDataset(corpus, tokenizer, max_seq_length=2)\n",
        "\n",
        "assert len(dataset) == 13\n",
        "\n",
        "tokens1 = torch.IntTensor(tokenizer.encode(corpus[0]))\n",
        "print(dataset[1], tokens1)\n",
        "for i in range(2):\n",
        "  assert (dataset[i+1][0] == tokens1[i*2+1:i*2+3]).all()\n",
        "  assert (dataset[i+1][2] == tokens1[i*2+2:i*2+4]).all()\n",
        "\n",
        "#Teste se sequência com tokens desconhecidos são ignorados\n",
        "tokenizer = IMDBTokenizer(max_tokens=3)\n",
        "tokenizer.create_vocab(corpus)\n",
        "\n",
        "dataset = IMDBDataset(corpus, tokenizer, max_seq_length=2)\n",
        "tokens2 = torch.IntTensor(tokenizer.encode(corpus[0]))\n",
        "print(dataset[0], tokens2)\n",
        "\n",
        "assert len(dataset) == 13\n",
        "assert (dataset[1][0] == tokens2[2:4]).all()\n",
        "assert (dataset[1][2] == torch.tensor([-100, -100])).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEpYX-qYh8Su"
      },
      "source": [
        "# 4) Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_nhhim6J8Jm"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  \n",
        "  def __init__(self, heads: int, embedding_dim: int = 50, test: bool = False):\n",
        "    super(MultiHeadSelfAttention, self).__init__()\n",
        "    \n",
        "    self.test = test\n",
        "    \n",
        "    self.heads = heads\n",
        "    assert embedding_dim % heads == 0, \"Dimensão de embedding deve ser divisível pelo número de cabeças\"\n",
        "\n",
        "    self.querry_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.key_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.value_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
        "  \n",
        "  def forward(self, input_embeddings, mask=None):\n",
        "    Q = self.querry_proj(input_embeddings)\n",
        "    K = self.key_proj(input_embeddings)\n",
        "    V = self.value_proj(input_embeddings)\n",
        "\n",
        "    Q = Q.reshape(Q.shape[0], Q.shape[1], self.heads, -1)\n",
        "    K = K.reshape(K.shape[0], K.shape[1], self.heads, -1)\n",
        "    V = V.reshape(V.shape[0], V.shape[1], self.heads, -1)\n",
        "\n",
        "    Q = Q.transpose(1, 2)\n",
        "    K = K.transpose(1, 2)\n",
        "    V = V.transpose(1, 2)\n",
        "\n",
        "    scores = Q @ torch.transpose(K, -2, -1)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores[mask == 0] = -1e8\n",
        "\n",
        "    attention_weigths = torch.softmax(scores, dim=-1)\n",
        "    \n",
        "    E = attention_weigths @ V\n",
        "\n",
        "\n",
        "    E = E.transpose(1, 2)\n",
        "\n",
        "    E = E.reshape(E.shape[0], E.shape[1], -1)\n",
        "\n",
        "    E = self.out_proj(E)\n",
        "\n",
        "    if self.test:\n",
        "      return E, attention_weigths\n",
        "    else:\n",
        "      return E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URCO6fdTKyXA"
      },
      "outputs": [],
      "source": [
        "m = MultiHeadSelfAttention(5)\n",
        "x = torch.arange(100).reshape(1,2,50).float()\n",
        "\n",
        "# Se máscara é fully-visible os resultados devem ser iguais\n",
        "mask = torch.tensor([[1, 1], [1, 1]])\n",
        "mask = mask.view(1, 1, 2, 2).expand(1, 5, 2, 2)\n",
        "assert torch.sum(m(x, mask=mask) - m(x)) < 1e-6\n",
        "\n",
        "# Se máscara é fully-visible para o segundo elemento, mas não para o primeiro\n",
        "mask = torch.tensor([[1, 0], [1, 1]])\n",
        "mask = mask.view(1, 1, 2, 2).expand(1, 5, 2, 2)\n",
        "assert torch.sum(torch.abs(m(x, mask=mask)[:, 0, :] - m(x)[:, 0, :])) > 1\n",
        "assert torch.sum(torch.abs(m(x, mask=mask)[:, 1, :] - m(x)[:, 1, :])) < 1e-6\n",
        "\n",
        "# Se máscara é fully-visible para o primeiro elemento, mas não para o segundo\n",
        "mask = torch.tensor([[1, 1], [1, 0]])\n",
        "mask = mask.view(1, 1, 2, 2).expand(1, 5, 2, 2)\n",
        "assert torch.sum(torch.abs(m(x, mask=mask)[:, 0, :] - m(x)[:, 0, :])) < 1e-6\n",
        "assert torch.sum(torch.abs(m(x, mask=mask)[:, 1, :] - m(x)[:, 1, :])) > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RloBgjSjYqSz",
        "outputId": "43e4d488-6cc1-4f4e-d17a-3dd67895467f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              " \n",
              "          [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "           [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]]),\n",
              " tensor([[1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.50, 0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.34, 0.33, 0.33, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.26, 0.25, 0.25, 0.24, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.21, 0.21, 0.20, 0.19, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.18, 0.18, 0.17, 0.16, 0.16, 0.15, 0.00, 0.00, 0.00, 0.00],\n",
              "         [0.16, 0.16, 0.15, 0.14, 0.14, 0.13, 0.13, 0.00, 0.00, 0.00],\n",
              "         [0.15, 0.14, 0.13, 0.13, 0.12, 0.12, 0.11, 0.10, 0.00, 0.00],\n",
              "         [0.14, 0.13, 0.12, 0.12, 0.11, 0.10, 0.10, 0.09, 0.09, 0.00],\n",
              "         [0.13, 0.12, 0.12, 0.11, 0.10, 0.10, 0.09, 0.08, 0.08, 0.07]],\n",
              "        grad_fn=<SliceBackward0>))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.set_printoptions(precision=2, sci_mode=False)\n",
        "m = MultiHeadSelfAttention(5, test=True)\n",
        "x = torch.arange(500).reshape(1,10,50).float()/500\n",
        "mask = torch.tril(torch.ones(10, 10))\n",
        "mask = mask.view(1, 1, 10, 10).expand(1, 5, 10, 10)\n",
        "mask, m(x, mask=mask)[1][0,0,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWElEr66h-iI"
      },
      "outputs": [],
      "source": [
        "class MyAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, max_seq_length: int, vocab_size: int, embedding_dim: int = 50, heads: int = 5, eos_token_id: int = None):\n",
        "    super(MyAttentionModel, self).__init__()\n",
        "\n",
        "    self.generating = False\n",
        "\n",
        "    self.eos_token_id = eos_token_id\n",
        "\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    self.heads = heads\n",
        "\n",
        "    self.causal_mask = nn.Parameter(data=torch.tril(torch.ones(max_seq_length, max_seq_length)), requires_grad=False)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.15)\n",
        "\n",
        "    self.tokens_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.positional_embeddings = nn.Parameter(data=torch.normal(0, 0.1, size=(max_seq_length, embedding_dim)))\n",
        "    \n",
        "    self.self_attention = MultiHeadSelfAttention(heads=heads, embedding_dim=embedding_dim)\n",
        "\n",
        "    self.feed_foward = nn.Sequential(\n",
        "        nn.Linear(embedding_dim, 4*embedding_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*embedding_dim, embedding_dim)\n",
        "    )\n",
        "\n",
        "    self.language_head = nn.Linear(embedding_dim, vocab_size)\n",
        "  \n",
        "  def forward(self, x, attention_mask=None):\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    input_embeddings = self.tokens_embeddings(x) + self.positional_embeddings[:seq_len, :]\n",
        "\n",
        "    if attention_mask is not None:\n",
        "      attention_mask = attention_mask.reshape(batch_size, 1, 1, seq_len).expand(-1, self.heads, seq_len, -1)\n",
        "      causal_mask = self.causal_mask[:seq_len, :seq_len].reshape(1, 1, seq_len, seq_len).expand(batch_size, self.heads, seq_len, seq_len)\n",
        "      mask = attention_mask * causal_mask\n",
        "    else:\n",
        "      mask = self.causal_mask[:seq_len, :seq_len].reshape(1, 1, seq_len, seq_len).expand(batch_size, self.heads, seq_len, seq_len)\n",
        "\n",
        "    E = self.self_attention(input_embeddings, mask=mask) \n",
        "    E = E + self.dropout(input_embeddings) # skip-connection\n",
        "\n",
        "    y = self.feed_foward(E)\n",
        "    y = y + self.dropout(E) # skip-connection\n",
        "    \n",
        "    if self.generating:\n",
        "      logits = self.language_head(y[:, -1, :])\n",
        "    else:\n",
        "      logits = self.language_head(y)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def generate(self, x, attention_mask=None, max_length=10):\n",
        "    self.generating = True\n",
        "\n",
        "    # Garantir que tem a dimensão do batch\n",
        "    if len(x.shape) < 2:\n",
        "      x = x.reshape(1, -1)\n",
        "\n",
        "    initial_input = x\n",
        "\n",
        "    text_out = self.__call__(x, attention_mask=attention_mask).argmax(dim=-1)\n",
        "    x = torch.hstack((x, text_out.reshape(1, -1)))\n",
        "    \n",
        "    while text_out.shape[0] <= max_length and text_out[-1].item() != self.eos_token_id:\n",
        "      \n",
        "      token_id = self.__call__(x, attention_mask=attention_mask).argmax(dim=-1)\n",
        "      text_out = torch.cat((text_out, token_id))\n",
        "\n",
        "      x = torch.hstack((x, text_out[-1].reshape(1, 1)))\n",
        "     \n",
        "      if attention_mask is not None:\n",
        "        attention_mask = torch.hstack((attention_mask, torch.tensor([[1]])))\n",
        "\n",
        "      if x.shape[1] > self.max_seq_length:\n",
        "        x = x[:,1:]\n",
        "        if attention_mask is not None:\n",
        "          attention_mask = attention_mask[:,1:]\n",
        "\n",
        "\n",
        "    self.generating = False\n",
        "    \n",
        "    return torch.cat((initial_input[0], text_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA04feFOlQet",
        "outputId": "e7ad2dec-354f-4e59-89fa-0fa91c6cc12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 10])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 10])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m = MyAttentionModel(13, 10, 5)\n",
        "x = torch.LongTensor([[1, 2, 3], [1, 2, 3]])\n",
        "print(m(x).shape)\n",
        "m.generating = True\n",
        "m(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9QVgXx6uYAC",
        "outputId": "fd8e8251-1d27-40d3-e25a-3a5d1d48e8c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 2, 3, 7, 7, 8, 8, 8, 1, 1, 1, 1, 1, 0, 2, 1])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.generate(torch.LongTensor([1, 2, 3, 2, 3, 7, 7]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ej48hqGTYJ2"
      },
      "source": [
        "# 5) Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGIRYtb_Ta4j"
      },
      "source": [
        "## 5.1) Hyperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA9bew2TTeSD"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 1e-3,\n",
        "    'epochs': 40,\n",
        "    'embedding_dim': 200,\n",
        "    'vocab_size': 5000,\n",
        "    'max_seq_length': 15\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7c_U28CTrfG"
      },
      "source": [
        "## 5.2) Datasets e Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNe60T8wHARY",
        "outputId": "df7c44e0-5db1-4d31-b7f8-271987bc25d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes | Train: 385,277 - Valid: 1,315,154\n",
            "Number of batches | Train: 3,010 - Valid: 2,631\n"
          ]
        }
      ],
      "source": [
        "tokenizer = IMDBTokenizer(max_tokens=hyperparams['vocab_size'])\n",
        "tokenizer.create_vocab(x_train)\n",
        "\n",
        "train_dataset = IMDBDataset(x_train, tokenizer, max_seq_length=hyperparams['max_seq_length'])\n",
        "valid_dataset = IMDBDataset(x_valid, tokenizer, max_seq_length=hyperparams['max_seq_length'], shifting_window=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=500)\n",
        "\n",
        "print(f'Dataset sizes | Train: {len(train_dataset):,} - Valid: {len(valid_dataset):,}')\n",
        "print(f'Number of batches | Train: {len(train_loader):,} - Valid: {len(valid_loader):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b505jUIlWvlz"
      },
      "source": [
        "## 5.3) Funções de treino e validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z4gJRClWyvZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_only_last(model, valid_dataloader, len_valid, criterion):\n",
        "  accuracy = 0\n",
        "  acc_loss = 0\n",
        "  model.eval()\n",
        "  model.generating = True\n",
        "  for i, (input_ids, attention_mask, output_ids, decoder_mask) in enumerate(valid_dataloader):\n",
        "    input_ids, attention_mask, output_ids, decoder_mask = input_ids.to(device), attention_mask.to(device), output_ids.to(device), decoder_mask.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask=attention_mask)\n",
        "    \n",
        "    loss = criterion(logits, output_ids[:, -1])\n",
        "    acc_loss += loss.item()\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    batch_correct = (preds == output_ids[:, -1])\n",
        "    accuracy += batch_correct.sum().item()\n",
        "\n",
        "  model.generating = False\n",
        "  return accuracy / len_valid, acc_loss / len(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crme1yqAmy3c"
      },
      "outputs": [],
      "source": [
        "def evaluate_sequence(model, valid_dataloader, len_valid, criterion):\n",
        "  accuracy = 0\n",
        "  acc_loss = 0\n",
        "  model.eval()\n",
        "  for i, (input_ids, attention_mask, output_ids, decoder_mask) in enumerate(valid_dataloader):\n",
        "    input_ids, attention_mask, output_ids, decoder_mask = input_ids.to(device), attention_mask.to(device), output_ids.to(device), decoder_mask.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids, attention_mask=attention_mask)\n",
        "    \n",
        "    loss = criterion(logits.transpose(-2, -1), output_ids)\n",
        "    acc_loss += loss.item()\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    batch_correct = (preds == output_ids)\n",
        "    accuracy += batch_correct.sum().item() / batch_correct.shape[1]\n",
        "\n",
        "  return accuracy / len_valid, acc_loss / len(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv99xFSzWzhI"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, optimizer, criterion, scheduler):\n",
        "  acc_loss = 0\n",
        "  model.train()\n",
        "  for i, (input_ids, attention_mask, output_ids, decoder_mask) in enumerate(train_dataloader):\n",
        "    input_ids, attention_mask, output_ids, decoder_mask = input_ids.to(device), attention_mask.to(device), output_ids.to(device), decoder_mask.to(device)\n",
        "    \n",
        "    logits = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = criterion(logits.transpose(-2, -1), output_ids)\n",
        "    acc_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "  \n",
        "    current_mean_loss = acc_loss / (i+1)\n",
        "  \n",
        "  return acc_loss / len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVkHVfrNeA3h"
      },
      "source": [
        "## 5.4) Tetes das métricas do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfsh6sE9eGvy",
        "outputId": "f158a094-e678-4b86-e266-06aadd1311bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before trainning metrics | Loss: 8.7016 - Perplexity: 6012.4105 - Accuracy: 0.0002\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "initial_accuracy, initial_loss = 0, 0\n",
        "for i in range(5):\n",
        "  model = MyAttentionModel(hyperparams['max_seq_length'], tokenizer.vocab_size, embedding_dim=hyperparams['embedding_dim']).to(device)\n",
        "  metrics = evaluate_only_last(model, valid_loader, len(valid_dataset), criterion)\n",
        "  initial_accuracy += metrics[0] / 5\n",
        "  initial_loss += metrics[1] / 5\n",
        "\n",
        "initial_perplexity = np.exp(initial_loss)\n",
        "print(f'Before trainning metrics | Loss: {initial_loss:.4f} - Perplexity: {initial_perplexity:.4f} - Accuracy: {initial_accuracy:.4f}')\n",
        "\n",
        "#Testando se métricas iniciais estão dentro do esperado\n",
        "assert (0.9 * tokenizer.vocab_size) <= initial_perplexity <= (1.25 * tokenizer.vocab_size)\n",
        "assert initial_accuracy <= (1.25 / tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "braj5SXTW48U"
      },
      "source": [
        "## 5.5) Laço de treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "f314242e1dcb4f869450693764aac93d",
            "758ba4493bd84a7ca36d7d24667f41db",
            "299b91b55b8440eaa7764c274445d76b",
            "78f5e014905b473a9b567f337624083e",
            "068e65add6f8400fb630c829e6768af1",
            "74f8faf4d16d4f9d89a280f0eead4801",
            "6e7d92fce7234787ba2b2ea04cb974ac",
            "7e6d947583154da3b00b2c2b622d8859",
            "06a9f1624d854ef8a57bfb8eab2de0c1",
            "8e84472ae74342d69377794c67914bb3",
            "90c27a815e154a60846bd2a4f6423347"
          ]
        },
        "id": "YRioiONRO8p9",
        "outputId": "86eedad4-1874-4c63-f26e-371056d0b252"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f314242e1dcb4f869450693764aac93d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 1.000\ttrain_loss: 5.389\ttrain_perplexity: 218.899\tval_loss: 4.859\tval_perplexity: 128.833\tval_acc: 0.166\t(101.30s - 3950.52s remaining)\n",
            "EPOCH: 2.000\ttrain_loss: 4.773\ttrain_perplexity: 118.229\tval_loss: 4.621\tval_perplexity: 101.612\tval_acc: 0.185\t(202.94s - 3855.92s remaining)\n",
            "EPOCH: 3.000\ttrain_loss: 4.628\ttrain_perplexity: 102.357\tval_loss: 4.516\tval_perplexity: 91.433\tval_acc: 0.193\t(303.68s - 3745.41s remaining)\n",
            "EPOCH: 4.000\ttrain_loss: 4.539\ttrain_perplexity: 93.598\tval_loss: 4.456\tval_perplexity: 86.147\tval_acc: 0.198\t(404.66s - 3641.92s remaining)\n",
            "EPOCH: 5.000\ttrain_loss: 4.478\ttrain_perplexity: 88.047\tval_loss: 4.420\tval_perplexity: 83.112\tval_acc: 0.202\t(504.05s - 3528.38s remaining)\n",
            "EPOCH: 6.000\ttrain_loss: 4.431\ttrain_perplexity: 84.034\tval_loss: 4.387\tval_perplexity: 80.367\tval_acc: 0.205\t(604.34s - 3424.57s remaining)\n",
            "EPOCH: 7.000\ttrain_loss: 4.393\ttrain_perplexity: 80.845\tval_loss: 4.363\tval_perplexity: 78.519\tval_acc: 0.207\t(703.39s - 3315.99s remaining)\n",
            "EPOCH: 8.000\ttrain_loss: 4.360\ttrain_perplexity: 78.261\tval_loss: 4.345\tval_perplexity: 77.107\tval_acc: 0.209\t(803.04s - 3212.15s remaining)\n",
            "EPOCH: 9.000\ttrain_loss: 4.331\ttrain_perplexity: 75.991\tval_loss: 4.327\tval_perplexity: 75.745\tval_acc: 0.210\t(903.09s - 3110.64s remaining)\n",
            "EPOCH: 10.000\ttrain_loss: 4.305\ttrain_perplexity: 74.071\tval_loss: 4.316\tval_perplexity: 74.864\tval_acc: 0.212\t(1001.74s - 3005.23s remaining)\n",
            "EPOCH: 11.000\ttrain_loss: 4.282\ttrain_perplexity: 72.371\tval_loss: 4.306\tval_perplexity: 74.180\tval_acc: 0.213\t(1101.43s - 2903.76s remaining)\n",
            "EPOCH: 12.000\ttrain_loss: 4.260\ttrain_perplexity: 70.834\tval_loss: 4.294\tval_perplexity: 73.226\tval_acc: 0.214\t(1200.43s - 2801.00s remaining)\n",
            "EPOCH: 13.000\ttrain_loss: 4.241\ttrain_perplexity: 69.458\tval_loss: 4.286\tval_perplexity: 72.695\tval_acc: 0.215\t(1300.00s - 2700.00s remaining)\n",
            "EPOCH: 14.000\ttrain_loss: 4.222\ttrain_perplexity: 68.172\tval_loss: 4.280\tval_perplexity: 72.215\tval_acc: 0.217\t(1399.67s - 2599.38s remaining)\n",
            "EPOCH: 15.000\ttrain_loss: 4.205\ttrain_perplexity: 67.020\tval_loss: 4.273\tval_perplexity: 71.733\tval_acc: 0.217\t(1498.59s - 2497.65s remaining)\n",
            "EPOCH: 16.000\ttrain_loss: 4.189\ttrain_perplexity: 65.925\tval_loss: 4.268\tval_perplexity: 71.371\tval_acc: 0.218\t(1598.25s - 2397.37s remaining)\n",
            "EPOCH: 17.000\ttrain_loss: 4.174\ttrain_perplexity: 64.943\tval_loss: 4.263\tval_perplexity: 71.045\tval_acc: 0.218\t(1697.18s - 2296.19s remaining)\n",
            "EPOCH: 18.000\ttrain_loss: 4.159\ttrain_perplexity: 63.982\tval_loss: 4.263\tval_perplexity: 71.016\tval_acc: 0.219\t(1797.01s - 2196.34s remaining)\n",
            "EPOCH: 19.000\ttrain_loss: 4.145\ttrain_perplexity: 63.100\tval_loss: 4.261\tval_perplexity: 70.902\tval_acc: 0.219\t(1896.51s - 2096.14s remaining)\n",
            "EPOCH: 19.990\ttrain_loss: 4.131\ttrain_perplexity: 62.237\t(1931.64s - 1933.58s remaining)"
          ]
        }
      ],
      "source": [
        "model = MyAttentionModel(hyperparams['max_seq_length'], tokenizer.vocab_size, embedding_dim=hyperparams['embedding_dim'], eos_token_id=tokenizer.eos_token_id).to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=hyperparams['learning_rate'], steps_per_epoch=len(train_loader), epochs=hyperparams['epochs'],anneal_strategy='linear', pct_start=0.05)\n",
        "\n",
        "log = Report(hyperparams['epochs'])\n",
        "best_loss = 100000\n",
        "\n",
        "for e in tqdm(range(hyperparams['epochs'])):\n",
        "  train_loss = train(model, train_loader, optim, criterion, scheduler)\n",
        "  log.record(pos=e+0.99, train_loss=train_loss, train_perplexity=np.exp(train_loss), end='\\r')\n",
        "  \n",
        "  valid_accuracy, valid_loss = evaluate_only_last(model, valid_loader, len(valid_dataset), criterion)\n",
        "  log.record(pos=e+0.99, val_loss=valid_loss, val_perplexity=np.exp(valid_loss), val_acc=valid_accuracy, end='\\r')\n",
        "  \n",
        "  if valid_loss < best_loss:\n",
        "    torch.save({'model_state_dict': model.cpu().state_dict(), 'tokenizer': tokenizer, 'hyperparams': hyperparams}, 'model_best.pt')\n",
        "    model.to(device)\n",
        "    best_loss = valid_loss\n",
        "  \n",
        "  log.report_avgs(e+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhgXOoHDO8p_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 9)\n",
        "#ax.set_ylim([3.95, 4.15])\n",
        "log.plot(['train_loss', 'val_loss'], ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgZLUpfJO8p_"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(20, 9)\n",
        "#ax.set_ylim([53, 65])\n",
        "log.plot(['train_perplexity', 'val_perplexity'], ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVNx7oaO8qA"
      },
      "source": [
        "# 6) Analisando o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pthDzFndO8qA"
      },
      "source": [
        "## 6.1) Carregando modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZSPY22zO8qA"
      },
      "outputs": [],
      "source": [
        "def load_model_and_tokenizer(file_name: str):\n",
        "  checkpoint = torch.load(file_name)\n",
        "  load_params = checkpoint['hyperparams']\n",
        "  load_tokenizer = checkpoint['tokenizer']\n",
        "  load_model = MyAttentionModel(load_params['max_seq_length'], load_tokenizer.vocab_size, embedding_dim=load_params['embedding_dim'])\n",
        "  load_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "  model_params = {k:v for k, v in load_params.items() if k in ['max_seq_length', 'vocab_size', 'embedding_dim']}\n",
        "  print(f'Model loaded | {model_params}')\n",
        "\n",
        "  return load_model, load_tokenizer, load_params['max_seq_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwc0bdviO8qB"
      },
      "outputs": [],
      "source": [
        "model_best, tokenizer_best, input_length = load_model_and_tokenizer('model_best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OUwkS8BO8qB"
      },
      "outputs": [],
      "source": [
        "print(model_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym-xlHkIO8qB"
      },
      "source": [
        "## 6.2) Métricas no dataset de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYbgRFq-O8qB"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "test_dataset = IMDBDataset(x_test, tokenizer_best, max_seq_length=input_length)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=500)\n",
        "\n",
        "test_accuracy1, test_loss1 = evaluate_only_last(model_best.to(device), test_loader, len(test_dataset), criterion)\n",
        "\n",
        "test_accuracy2, test_loss2 = evaluate_sequence(model_best.to(device), test_loader, len(test_dataset), criterion)\n",
        "\n",
        "print(f'Final model scores | Loss: {test_loss1:.5f} - Perplexity: {np.exp(test_loss1):.5f} - Accuracy: {test_accuracy1:.5f}')\n",
        "print(f'Final model scores | Loss: {test_loss2:.5f} - Perplexity: {np.exp(test_loss2):.5f} - Accuracy: {test_accuracy2:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edIi_Y2CO8qC"
      },
      "source": [
        "## 6.3) Geração de textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBhi1eTIO8qC"
      },
      "outputs": [],
      "source": [
        "model_best.cpu()\n",
        "def generate_text(tknz: Type[IMDBTokenizer], seed: str, gen_length: int = 50):\n",
        "  input = torch.tensor(tknz.encode('<sos> ' + seed))\n",
        "  pred = model_best.generate(input, max_length=gen_length).tolist()\n",
        "  out_text = tknz.decode(pred)\n",
        "\n",
        "  return out_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwbFNm1-O8qC"
      },
      "outputs": [],
      "source": [
        "generate_text(tokenizer_best, 'The movie was really good, but')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rneJayNWO8qD"
      },
      "outputs": [],
      "source": [
        "generate_text(tokenizer_best, 'This was not the worst movie I\\'ve')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvEsya4AO8qD"
      },
      "outputs": [],
      "source": [
        "generate_text(tokenizer_best, 'Today I was walking on the street and then')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVY7KbquO8qD"
      },
      "outputs": [],
      "source": [
        "generate_text(tokenizer_best, 'I walked my dog by the park today when')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9H3MQZ_O8qE"
      },
      "source": [
        "## 6.4) Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMJo3fCGO8qE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "valid_dataset = IMDBDataset(x_valid, tokenizer_best, max_seq_length=input_length)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset)//5)\n",
        "\n",
        "preds, ground_truth = torch.IntTensor([]), torch.IntTensor([])\n",
        "for x, att, y, _ in valid_loader:\n",
        "  logits = model_best(x)\n",
        "  preds = torch.concat((preds, logits.argmax(dim=1)))\n",
        "  ground_truth = torch.concat((ground_truth, y))\n",
        "\n",
        "cfm = confusion_matrix(ground_truth.numpy(), preds.numpy())\n",
        "del preds, x, y, ground_truth, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoE3iClsO8qE"
      },
      "outputs": [],
      "source": [
        "num_labels = 35\n",
        "\n",
        "filter_cfm = cfm[:num_labels,:num_labels]\n",
        "filter_cfm = filter_cfm/filter_cfm.sum(axis=1)\n",
        "\n",
        "labels = list(tokenizer_best.vocab.keys())[:num_labels]\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=filter_cfm, display_labels=labels)\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(15, 15)\n",
        "disp.plot(include_values=False, ax=ax, values_format='.2f')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "068e65add6f8400fb630c829e6768af1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a9f1624d854ef8a57bfb8eab2de0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "299b91b55b8440eaa7764c274445d76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e6d947583154da3b00b2c2b622d8859",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06a9f1624d854ef8a57bfb8eab2de0c1",
            "value": 19
          }
        },
        "6e7d92fce7234787ba2b2ea04cb974ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f8faf4d16d4f9d89a280f0eead4801": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "758ba4493bd84a7ca36d7d24667f41db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f8faf4d16d4f9d89a280f0eead4801",
            "placeholder": "​",
            "style": "IPY_MODEL_6e7d92fce7234787ba2b2ea04cb974ac",
            "value": " 48%"
          }
        },
        "78f5e014905b473a9b567f337624083e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e84472ae74342d69377794c67914bb3",
            "placeholder": "​",
            "style": "IPY_MODEL_90c27a815e154a60846bd2a4f6423347",
            "value": " 19/40 [31:36&lt;34:48, 99.46s/it]"
          }
        },
        "7e6d947583154da3b00b2c2b622d8859": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e84472ae74342d69377794c67914bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c27a815e154a60846bd2a4f6423347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f314242e1dcb4f869450693764aac93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_758ba4493bd84a7ca36d7d24667f41db",
              "IPY_MODEL_299b91b55b8440eaa7764c274445d76b",
              "IPY_MODEL_78f5e014905b473a9b567f337624083e"
            ],
            "layout": "IPY_MODEL_068e65add6f8400fb630c829e6768af1"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
