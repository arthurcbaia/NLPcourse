{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "e14f5479-3470-4177-ff26-6bf16c560f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur Baia\n"
          ]
        }
      ],
      "source": [
        "nome = 'Arthur Baia'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída). \n",
        "- Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9f3PfifAwpU",
        "outputId": "bd62c9e1-6ab8-4c09-e597-c488369928a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Sep  9 11:37:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   52C    P5    11W /  N/A |    635MiB /  5944MiB |      6%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      2105      G   /usr/lib/xorg/Xorg                 70MiB |\n",
            "|    0   N/A  N/A      3348      G   /usr/lib/xorg/Xorg                174MiB |\n",
            "|    0   N/A  N/A      3521      G   /usr/bin/gnome-shell               44MiB |\n",
            "|    0   N/A  N/A      3991      G   /usr/lib/firefox/firefox          178MiB |\n",
            "|    0   N/A  N/A      8043      G   ...821948483967496742,131072       28MiB |\n",
            "|    0   N/A  N/A     12415      G   ...RendererForSitePerProcess      127MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTCe2i7AtoV",
        "outputId": "a6126268-9a7c-421b-d002-f89b0db1a57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "add0bd18-4e07-414d-a6ea-4d65dbfd1e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "6cacd89f-7e20-433d-b669-291273cb0ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "I can't say this is the worst film of all time, but only because there are still some movies I haven\n",
            "Oh my, this was the worst reunion movie I have ever seen. (That is saying a lot.) I am ashamed of wa\n",
            "Great Woody Allen? No. Good Woody Allen? Definitely. I found myself, along with the audience in atte\n",
            "3 últimas amostras treino:\n",
            "Wesley Snipes is James Dial, an assassin for hire, agent of the CIA and pure bad-ass special operati\n",
            "This film is more about how children make sense of the world around them, and how they (and we) use \n",
            "First off, I dislike almost all Neil Simon movies. But there is something about this that is unique,\n",
            "3 primeiras amostras validação:\n",
            "I watched this....let me rephrase...suffered through this because I'm a fan of Eva's. I don't think \n",
            "Well, on it's credit side (if it can be said to have one), Timothy Hines DID manage to capture the o\n",
            "Well made and stylish while still ultimately making sense this thriller would work better for non gi\n",
            "3 últimas amostras validação:\n",
            "A real disappointment from the great visual master Ridley Scott. G.I. Jane tells the story of a firs\n",
            "I remember this show from Swedish television. I was only 7 years of age and it scared me beyond beli\n",
            "I couldn't relate to this film. It failed to engage me either intellectually, emotionally or aesthet\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sekvvxEteayf"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self, max_vocab_token=1000):\n",
        "\n",
        "        self.max_vocab_tokens = max_vocab_token\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        # Escreva aqui seu código.\n",
        "        return [self.vocab[word] if word in self.vocab else -1 for word in self.tokenize(text)]\n",
        "\n",
        "    def decode(self, tokens: List[int]):\n",
        "        # Escreva aqui seu código.\n",
        "        return ' '.join([self.vocab_inv[token] for token in tokens])\n",
        "\n",
        "    def create_vocab(self, texts: List[str]):\n",
        "        L = [word for phrase in list(map(self.tokenize, texts))\n",
        "             for word in phrase]\n",
        "        k = self.max_vocab_tokens\n",
        "        def vocab(L, k): return {value: key for key, value in enumerate(\n",
        "            dict(collections.Counter(L).most_common(k)))}\n",
        "        self.vocab = vocab(L, k)\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\"\n",
        "        Convert string to a list of tokens (i.e., words).\n",
        "        This function lower cases everything and removes punctuation.\n",
        "        \"\"\"\n",
        "        # Escreva aqui seu código.\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-1]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_tokenizer():\n",
        "    phrase = 'a cat walks in the bad.'\n",
        "    vocab = Tokenizer(len(phrase.split())+1)\n",
        "    vocab_ = vocab.create_vocab(['a cat walks in the bad.'])\n",
        "    vocab.encode('vasco')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = Tokenizer(3000)\n",
        "vocab.create_vocab(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBdataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, corpus: List[str], vocab, context_size=9):\n",
        "        x = [\n",
        "            [vocab.encode(text)[i:i+context_size] for i in range(len(vocab.encode(text))-context_size)]\n",
        "            for text in corpus\n",
        "            if -1 not in vocab.encode(text) and len(vocab.encode(text)) > context_size\n",
        "        ]\n",
        "        self.x = list(itertools.chain.from_iterable(x))\n",
        "        print(len(self.x))\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx])[:-1], torch.tensor(self.y[idx])[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "imdb_train_test = IMDBdataset(['I really liked this movie, it shows the history of a  Doctor called Joe', 'The movie is pretty good'], vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2928\n"
          ]
        }
      ],
      "source": [
        "imdb_train = IMDBdataset(x_train, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "imdb_dataloader = DataLoader(imdb_train, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim*context_size, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = torch.log_softmax(x, dim=1)\n",
        "        return x\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
