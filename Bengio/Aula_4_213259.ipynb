{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOdQB41_4ZxG",
        "outputId": "e14f5479-3470-4177-ff26-6bf16c560f67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur Baia\n"
          ]
        }
      ],
      "source": [
        "nome = 'Arthur Baia'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DBb0-Klwf2"
      },
      "source": [
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída). \n",
        "- Este dataset já possui um tamanho razoável e é bem provável que você vai precisar rodar seus experimentos com GPU.\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9f3PfifAwpU",
        "outputId": "bd62c9e1-6ab8-4c09-e597-c488369928a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Sep 18 11:05:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   49C    P5    11W /  N/A |    615MiB /  5944MiB |     22%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1900      G   /usr/lib/xorg/Xorg                 70MiB |\n",
            "|    0   N/A  N/A      3090      G   /usr/lib/xorg/Xorg                176MiB |\n",
            "|    0   N/A  N/A      3264      G   /usr/bin/gnome-shell               44MiB |\n",
            "|    0   N/A  N/A      4361      G   ...RendererForSitePerProcess      115MiB |\n",
            "|    0   N/A  N/A      5545      G   /usr/lib/firefox/firefox          173MiB |\n",
            "|    0   N/A  N/A      6935      G   ...860701054519179842,131072       24MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTCe2i7AtoV",
        "outputId": "a6126268-9a7c-421b-d002-f89b0db1a57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset \n",
        "\n",
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "add0bd18-4e07-414d-a6ea-4d65dbfd1e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "6cacd89f-7e20-433d-b669-291273cb0ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "Really, I can't believe that I spent $5 on this movie. I am a huge zombie fanatic and thought the mo\n",
            "You know a movie will not go well when John Carradine narrates (a.k.a. reads the script & plot synop\n",
            "I must give How She Move a near-perfect rating because the content is truly great. As a previous rev\n",
            "3 últimas amostras treino:\n",
            "I don't know if I'm just weird, but I thoroughly enjoyed this film. <br /><br />Return to Cabin by t\n",
            "I recently got the chance to view \"The Waterdance\", and quite liked it. I don't really understand wh\n",
            "As it is in Heaven {SPOILER WARNING)<br /><br />This was a great human drama that stimulated my emot\n",
            "3 primeiras amostras validação:\n",
            "I don't believe this was an acting challenge for Richard Harris. This was an uncomplicated plot, yet\n",
            "Wow, I loved this film. It may not have had the funding and advertising that the latest hollywood bl\n",
            "After viewing the film, I was truly shocked to see such a high rating on IMDb.<br /><br />'The Fanta\n",
            "3 últimas amostras validação:\n",
            "The movie's storyline is pat and quaint. Two women travel through the middle east and discover thems\n",
            "I have to admit that I went into Fever Pitch with low expectations. It's no huge revelation for me t\n",
            "Gurinda Chada's semi-autobiographical film (2002) is a gentle, poignant comedy set in the ethnically\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "random.shuffle(x_train)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x in x_train[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x in x_train[-3:]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x in x_valid[:3]:\n",
        "    print(x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x in x_valid[-3:]:\n",
        "    print(x[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "unk = '<UNK>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sekvvxEteayf"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "\n",
        "    def __init__(self, max_vocab_token=1000):\n",
        "\n",
        "        self.max_vocab_tokens = max_vocab_token\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        # Escreva aqui seu código.\n",
        "        return [self.vocab[word] if word in self.vocab else unk for word in self.tokenize(text)]\n",
        "\n",
        "    def decode(self, tokens: List[int]):\n",
        "        # Escreva aqui seu código.\n",
        "        return ' '.join([self.vocab_inv[token] for token in tokens])\n",
        "\n",
        "    def create_vocab(self, texts: List[str]):\n",
        "        L = [word for phrase in list(map(self.tokenize, texts))\n",
        "             for word in phrase]\n",
        "        k = self.max_vocab_tokens\n",
        "        def vocab(L, k): return {value: key for key, value in enumerate(\n",
        "            dict(collections.Counter(L).most_common(k)))}\n",
        "        self.vocab = vocab(L, k)\n",
        "        self.vocab_inv = {v: k for k, v in self.vocab.items()}\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\"\n",
        "        Convert string to a list of tokens (i.e., words).\n",
        "        This function lower cases everything and removes punctuation.\n",
        "        \"\"\"\n",
        "        # Escreva aqui seu código.\n",
        "        return re.findall(r\"\\w+|[^\\w\\s]\", text.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6]\n",
            "a cat walks in the bad .\n",
            "a cat walks .\n"
          ]
        }
      ],
      "source": [
        "def test_tokenizer():\n",
        "    phrase = 'a cat walks in the bad.'\n",
        "    vocab = Tokenizer(len(phrase.split())+1)\n",
        "    vocab_ = vocab.create_vocab([phrase])\n",
        "    print(vocab.encode(phrase))\n",
        "    print(vocab.decode(vocab.encode(phrase)))\n",
        "    # assert vocab.decode(vocab.encode(phrase)) == phrase\n",
        "    print(vocab.decode(vocab.encode('a cat walks.')))\n",
        "test_tokenizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IMDBdataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, corpus: List[str], vocab, context_size=9):\n",
        "        data = []\n",
        "        for text in corpus:\n",
        "            tokens = vocab.encode(text)\n",
        "            data.extend([\n",
        "                tokens[i:i+context_size+1] for i in range(len(tokens)-context_size)\n",
        "                if unk not in tokens[i:i+context_size+1] and len(tokens) > context_size\n",
        "            ])\n",
        "        self.data = torch.tensor(data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx][:-1], self.data[idx][-1].long()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to test the dataset\"\"\"\n",
        "def test_dataset():\n",
        "    vocab = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    vocab.create_vocab(test_corpus)\n",
        "    unseen_tokens = ['aprendendo sobre o modelo n-grama']\n",
        "    dataset_ = IMDBdataset(unseen_tokens, vocab, 3)\n",
        "    assert len(dataset_) == 0\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, vocab, context_size=context_size)\n",
        "    assert len(dataset_) == 8\n",
        "    assert len(dataset_.__getitem__(0)[0].tolist()) == context_size\n",
        "    assert len([dataset_.__getitem__(0)[1].tolist()]) == 1\n",
        "    assert dataset_.__getitem__(0)[1].tolist() == vocab.encode('in')[0]\n",
        "    assert dataset_.__getitem__(0)[0].tolist() == vocab.encode('a cat walks')\n",
        "\n",
        "\n",
        "test_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"function to test the dataloader\"\"\"\n",
        "def test_dataloader():\n",
        "    vocab = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    vocab.create_vocab(test_corpus)\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, vocab, context_size=context_size)\n",
        "    dataloader_ = torch.utils.data.DataLoader(dataset_, batch_size=2, shuffle=True)\n",
        "    for batch in dataloader_:\n",
        "        assert len(batch[0].shape) == 2\n",
        "        assert len(batch[1].shape) == 1\n",
        "        assert batch[0].shape[0] == batch[1].shape[0]\n",
        "        assert batch[0].shape[1] == context_size\n",
        "        break\n",
        "test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size*embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = torch.relu(x)\n",
        "        return torch.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to test the model\"\"\"\n",
        "def test_model():\n",
        "    vocab = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    vocab.create_vocab(test_corpus)\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, vocab, context_size=context_size)\n",
        "    dataloader_ = torch.utils.data.DataLoader(dataset_, batch_size=2, shuffle=True)\n",
        "    model = BengioModel(len(vocab.vocab), 10, 10, context_size)\n",
        "    for batch in dataloader_:\n",
        "        output = model(batch[0])\n",
        "        assert output.shape == (dataloader_.batch_size, len(vocab.vocab))\n",
        "        break\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to train the model, which returns the train loss\"\"\"\n",
        "def train(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x,y in train_dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_func(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to evaluate the model, which returns the validation loss and accuracy\"\"\"\n",
        "def evaluate(model, num_examples, valid_dataloader, loss_func):\n",
        "\n",
        "    correct = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    for x, y in valid_dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          logits = model(x)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        loss = loss_func(logits, y)\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        correct += (preds==y).sum().item()\n",
        "\n",
        "    return (correct / num_examples), (val_loss / len(valid_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to test the training and evaluation functions\"\"\"\n",
        "def test_train_eval():\n",
        "    vocab = Tokenizer()\n",
        "    test_corpus = ['a cat walks in the bad.', 'a dog walks in the good.']\n",
        "    vocab.create_vocab(test_corpus)\n",
        "    context_size = 3\n",
        "    dataset_ = IMDBdataset(test_corpus, vocab, context_size=context_size)\n",
        "    dataloader_ = torch.utils.data.DataLoader(dataset_, batch_size=2, shuffle=True)\n",
        "    model = BengioModel(len(vocab.vocab), 10, 10, context_size)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    train_loss = train(model, dataloader_, loss_func, optimizer)\n",
        "    valid_loss, accuracy = evaluate(model, dataloader_, loss_func)\n",
        "    assert train_loss > 0\n",
        "    assert valid_loss > 0\n",
        "    assert accuracy > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 0.0001\n",
        "vocab = Tokenizer()\n",
        "vocab.create_vocab(x_train)\n",
        "context_size = 5\n",
        "dataset_train = IMDBdataset(x_train, vocab, context_size=context_size)\n",
        "dataset_valid = IMDBdataset(x_valid, vocab, context_size=context_size)\n",
        "num_examples = len(dataset_valid)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(dataset_valid, batch_size=32, shuffle=True)\n",
        "model = BengioModel(len(vocab.vocab), 100, 100, context_size)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to(device)\n",
        "loss_func.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pré treino; Validation Loss: 6.899; Perplexity: 991.464; Accuracy: 0.001\n",
            "Epoch: 1; Train Loss: 4.260; Perplexity: 70.795; Validation Loss: 3.991; Accuracy: 0.293\n",
            "Epoch: 2; Train Loss: 3.906; Perplexity: 49.720; Validation Loss: 3.870; Accuracy: 0.310\n",
            "Epoch: 3; Train Loss: 3.819; Perplexity: 45.569; Validation Loss: 3.815; Accuracy: 0.316\n",
            "Epoch: 4; Train Loss: 3.772; Perplexity: 43.483; Validation Loss: 3.783; Accuracy: 0.321\n",
            "Epoch: 5; Train Loss: 3.742; Perplexity: 42.176; Validation Loss: 3.760; Accuracy: 0.325\n",
            "Epoch: 6; Train Loss: 3.720; Perplexity: 41.248; Validation Loss: 3.743; Accuracy: 0.328\n",
            "Epoch: 7; Train Loss: 3.702; Perplexity: 40.538; Validation Loss: 3.731; Accuracy: 0.329\n",
            "Epoch: 8; Train Loss: 3.688; Perplexity: 39.970; Validation Loss: 3.720; Accuracy: 0.331\n",
            "Epoch: 9; Train Loss: 3.676; Perplexity: 39.505; Validation Loss: 3.712; Accuracy: 0.332\n",
            "Epoch: 10; Train Loss: 3.666; Perplexity: 39.110; Validation Loss: 3.704; Accuracy: 0.333\n",
            "Epoch: 11; Train Loss: 3.658; Perplexity: 38.766; Validation Loss: 3.697; Accuracy: 0.334\n",
            "Epoch: 12; Train Loss: 3.649; Perplexity: 38.431; Validation Loss: 3.691; Accuracy: 0.335\n",
            "Epoch: 13; Train Loss: 3.642; Perplexity: 38.160; Validation Loss: 3.686; Accuracy: 0.335\n",
            "Epoch: 14; Train Loss: 3.636; Perplexity: 37.924; Validation Loss: 3.682; Accuracy: 0.336\n",
            "Epoch: 15; Train Loss: 3.630; Perplexity: 37.712; Validation Loss: 3.678; Accuracy: 0.336\n",
            "Epoch: 16; Train Loss: 3.625; Perplexity: 37.519; Validation Loss: 3.674; Accuracy: 0.337\n",
            "Epoch: 17; Train Loss: 3.620; Perplexity: 37.345; Validation Loss: 3.671; Accuracy: 0.338\n",
            "Epoch: 18; Train Loss: 3.616; Perplexity: 37.181; Validation Loss: 3.668; Accuracy: 0.339\n",
            "Epoch: 19; Train Loss: 3.612; Perplexity: 37.035; Validation Loss: 3.665; Accuracy: 0.338\n",
            "Epoch: 20; Train Loss: 3.608; Perplexity: 36.897; Validation Loss: 3.663; Accuracy: 0.339\n",
            "Epoch: 21; Train Loss: 3.605; Perplexity: 36.769; Validation Loss: 3.660; Accuracy: 0.339\n",
            "Epoch: 22; Train Loss: 3.601; Perplexity: 36.649; Validation Loss: 3.658; Accuracy: 0.340\n",
            "Epoch: 23; Train Loss: 3.598; Perplexity: 36.533; Validation Loss: 3.656; Accuracy: 0.340\n",
            "Epoch: 24; Train Loss: 3.595; Perplexity: 36.429; Validation Loss: 3.655; Accuracy: 0.340\n",
            "Epoch: 25; Train Loss: 3.593; Perplexity: 36.330; Validation Loss: 3.652; Accuracy: 0.341\n",
            "Epoch: 26; Train Loss: 3.590; Perplexity: 36.234; Validation Loss: 3.651; Accuracy: 0.341\n",
            "Epoch: 27; Train Loss: 3.588; Perplexity: 36.146; Validation Loss: 3.649; Accuracy: 0.341\n",
            "Epoch: 28; Train Loss: 3.585; Perplexity: 36.058; Validation Loss: 3.648; Accuracy: 0.341\n",
            "Epoch: 29; Train Loss: 3.583; Perplexity: 35.980; Validation Loss: 3.647; Accuracy: 0.341\n",
            "Epoch: 30; Train Loss: 3.581; Perplexity: 35.902; Validation Loss: 3.646; Accuracy: 0.341\n",
            "Epoch: 31; Train Loss: 3.579; Perplexity: 35.826; Validation Loss: 3.645; Accuracy: 0.342\n",
            "Epoch: 32; Train Loss: 3.577; Perplexity: 35.758; Validation Loss: 3.643; Accuracy: 0.342\n",
            "Epoch: 33; Train Loss: 3.575; Perplexity: 35.688; Validation Loss: 3.642; Accuracy: 0.342\n",
            "Epoch: 34; Train Loss: 3.573; Perplexity: 35.622; Validation Loss: 3.641; Accuracy: 0.342\n",
            "Epoch: 35; Train Loss: 3.571; Perplexity: 35.559; Validation Loss: 3.640; Accuracy: 0.342\n",
            "Epoch: 36; Train Loss: 3.569; Perplexity: 35.498; Validation Loss: 3.639; Accuracy: 0.342\n",
            "Epoch: 37; Train Loss: 3.568; Perplexity: 35.439; Validation Loss: 3.639; Accuracy: 0.342\n",
            "Epoch: 38; Train Loss: 3.566; Perplexity: 35.385; Validation Loss: 3.637; Accuracy: 0.342\n",
            "Epoch: 39; Train Loss: 3.565; Perplexity: 35.330; Validation Loss: 3.636; Accuracy: 0.343\n",
            "Epoch: 40; Train Loss: 3.563; Perplexity: 35.279; Validation Loss: 3.635; Accuracy: 0.343\n",
            "Epoch: 41; Train Loss: 3.562; Perplexity: 35.227; Validation Loss: 3.635; Accuracy: 0.343\n",
            "Epoch: 42; Train Loss: 3.560; Perplexity: 35.179; Validation Loss: 3.634; Accuracy: 0.343\n",
            "Epoch: 43; Train Loss: 3.559; Perplexity: 35.129; Validation Loss: 3.633; Accuracy: 0.343\n",
            "Epoch: 44; Train Loss: 3.558; Perplexity: 35.084; Validation Loss: 3.633; Accuracy: 0.344\n",
            "Epoch: 45; Train Loss: 3.557; Perplexity: 35.040; Validation Loss: 3.631; Accuracy: 0.344\n",
            "Epoch: 46; Train Loss: 3.555; Perplexity: 34.999; Validation Loss: 3.631; Accuracy: 0.344\n",
            "Epoch: 47; Train Loss: 3.554; Perplexity: 34.955; Validation Loss: 3.630; Accuracy: 0.344\n",
            "Epoch: 48; Train Loss: 3.553; Perplexity: 34.915; Validation Loss: 3.631; Accuracy: 0.343\n",
            "Epoch: 49; Train Loss: 3.552; Perplexity: 34.877; Validation Loss: 3.629; Accuracy: 0.344\n",
            "Epoch: 50; Train Loss: 3.551; Perplexity: 34.836; Validation Loss: 3.629; Accuracy: 0.344\n",
            "Epoch: 51; Train Loss: 3.550; Perplexity: 34.800; Validation Loss: 3.628; Accuracy: 0.344\n",
            "Epoch: 52; Train Loss: 3.549; Perplexity: 34.762; Validation Loss: 3.628; Accuracy: 0.344\n",
            "Epoch: 53; Train Loss: 3.548; Perplexity: 34.728; Validation Loss: 3.626; Accuracy: 0.345\n",
            "Epoch: 54; Train Loss: 3.547; Perplexity: 34.694; Validation Loss: 3.627; Accuracy: 0.345\n",
            "Epoch: 55; Train Loss: 3.546; Perplexity: 34.660; Validation Loss: 3.625; Accuracy: 0.345\n",
            "Epoch: 56; Train Loss: 3.545; Perplexity: 34.629; Validation Loss: 3.626; Accuracy: 0.345\n",
            "Epoch: 57; Train Loss: 3.544; Perplexity: 34.596; Validation Loss: 3.625; Accuracy: 0.345\n",
            "Epoch: 58; Train Loss: 3.543; Perplexity: 34.564; Validation Loss: 3.625; Accuracy: 0.344\n",
            "Epoch: 59; Train Loss: 3.542; Perplexity: 34.532; Validation Loss: 3.624; Accuracy: 0.345\n",
            "Epoch: 60; Train Loss: 3.541; Perplexity: 34.505; Validation Loss: 3.624; Accuracy: 0.345\n",
            "Epoch: 61; Train Loss: 3.540; Perplexity: 34.473; Validation Loss: 3.624; Accuracy: 0.345\n",
            "Epoch: 62; Train Loss: 3.539; Perplexity: 34.444; Validation Loss: 3.623; Accuracy: 0.345\n",
            "Epoch: 63; Train Loss: 3.539; Perplexity: 34.419; Validation Loss: 3.622; Accuracy: 0.345\n",
            "Epoch: 64; Train Loss: 3.538; Perplexity: 34.392; Validation Loss: 3.622; Accuracy: 0.345\n",
            "Epoch: 65; Train Loss: 3.537; Perplexity: 34.365; Validation Loss: 3.623; Accuracy: 0.345\n",
            "Epoch: 66; Train Loss: 3.536; Perplexity: 34.340; Validation Loss: 3.622; Accuracy: 0.345\n",
            "Epoch: 67; Train Loss: 3.535; Perplexity: 34.312; Validation Loss: 3.622; Accuracy: 0.346\n",
            "Epoch: 68; Train Loss: 3.535; Perplexity: 34.291; Validation Loss: 3.621; Accuracy: 0.345\n",
            "Epoch: 69; Train Loss: 3.534; Perplexity: 34.264; Validation Loss: 3.620; Accuracy: 0.346\n",
            "Epoch: 70; Train Loss: 3.533; Perplexity: 34.239; Validation Loss: 3.622; Accuracy: 0.346\n",
            "Epoch: 71; Train Loss: 3.533; Perplexity: 34.218; Validation Loss: 3.620; Accuracy: 0.346\n",
            "Epoch: 72; Train Loss: 3.532; Perplexity: 34.195; Validation Loss: 3.620; Accuracy: 0.346\n",
            "Epoch: 73; Train Loss: 3.531; Perplexity: 34.172; Validation Loss: 3.620; Accuracy: 0.346\n",
            "Epoch: 74; Train Loss: 3.531; Perplexity: 34.149; Validation Loss: 3.619; Accuracy: 0.345\n",
            "Epoch: 75; Train Loss: 3.530; Perplexity: 34.126; Validation Loss: 3.619; Accuracy: 0.346\n",
            "Epoch: 76; Train Loss: 3.530; Perplexity: 34.108; Validation Loss: 3.619; Accuracy: 0.346\n",
            "Epoch: 77; Train Loss: 3.529; Perplexity: 34.089; Validation Loss: 3.618; Accuracy: 0.346\n",
            "Epoch: 78; Train Loss: 3.528; Perplexity: 34.063; Validation Loss: 3.619; Accuracy: 0.346\n",
            "Epoch: 79; Train Loss: 3.528; Perplexity: 34.046; Validation Loss: 3.618; Accuracy: 0.346\n",
            "Epoch: 80; Train Loss: 3.527; Perplexity: 34.023; Validation Loss: 3.619; Accuracy: 0.346\n",
            "Epoch: 81; Train Loss: 3.526; Perplexity: 34.005; Validation Loss: 3.618; Accuracy: 0.346\n",
            "Epoch: 82; Train Loss: 3.526; Perplexity: 33.983; Validation Loss: 3.618; Accuracy: 0.345\n",
            "Epoch: 83; Train Loss: 3.525; Perplexity: 33.967; Validation Loss: 3.617; Accuracy: 0.347\n",
            "Epoch: 84; Train Loss: 3.525; Perplexity: 33.946; Validation Loss: 3.617; Accuracy: 0.346\n",
            "Epoch: 85; Train Loss: 3.524; Perplexity: 33.928; Validation Loss: 3.617; Accuracy: 0.347\n",
            "Epoch: 86; Train Loss: 3.524; Perplexity: 33.911; Validation Loss: 3.617; Accuracy: 0.346\n",
            "Epoch: 87; Train Loss: 3.523; Perplexity: 33.893; Validation Loss: 3.617; Accuracy: 0.346\n",
            "Epoch: 88; Train Loss: 3.523; Perplexity: 33.872; Validation Loss: 3.616; Accuracy: 0.346\n",
            "Epoch: 89; Train Loss: 3.522; Perplexity: 33.856; Validation Loss: 3.617; Accuracy: 0.346\n",
            "Epoch: 90; Train Loss: 3.522; Perplexity: 33.839; Validation Loss: 3.616; Accuracy: 0.347\n",
            "Epoch: 91; Train Loss: 3.521; Perplexity: 33.822; Validation Loss: 3.616; Accuracy: 0.347\n",
            "Epoch: 92; Train Loss: 3.521; Perplexity: 33.808; Validation Loss: 3.616; Accuracy: 0.347\n",
            "Epoch: 93; Train Loss: 3.520; Perplexity: 33.791; Validation Loss: 3.616; Accuracy: 0.347\n",
            "Epoch: 94; Train Loss: 3.520; Perplexity: 33.775; Validation Loss: 3.615; Accuracy: 0.347\n",
            "Epoch: 95; Train Loss: 3.519; Perplexity: 33.759; Validation Loss: 3.616; Accuracy: 0.347\n",
            "Epoch: 96; Train Loss: 3.519; Perplexity: 33.741; Validation Loss: 3.615; Accuracy: 0.346\n",
            "Epoch: 97; Train Loss: 3.518; Perplexity: 33.726; Validation Loss: 3.615; Accuracy: 0.347\n",
            "Epoch: 98; Train Loss: 3.518; Perplexity: 33.713; Validation Loss: 3.615; Accuracy: 0.347\n",
            "Epoch: 99; Train Loss: 3.517; Perplexity: 33.699; Validation Loss: 3.615; Accuracy: 0.347\n",
            "Epoch: 100; Train Loss: 3.517; Perplexity: 33.685; Validation Loss: 3.615; Accuracy: 0.347\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "epochs = 100\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "accuracy_train = []\n",
        "perplexities = []\n",
        "accuracy, valid_loss = evaluate(model, num_examples,valid_dataloader, loss_func)\n",
        "print(f\"Pré treino; Validation Loss: {valid_loss:.3f}; Perplexity: {np.exp(valid_loss):.3f}; Accuracy: {accuracy:.3f}\")\n",
        "for t in range(epochs):\n",
        "  train_loss = train(model, train_dataloader, loss_func, optimizer)\n",
        "  train_losses.append(train_loss)\n",
        "  accuracy, valid_loss = evaluate(model, num_examples, valid_dataloader, loss_func)\n",
        "  valid_losses.append(valid_loss)\n",
        "  accuracy_train.append(accuracy)\n",
        "  perplexities.append(np.exp(train_loss))\n",
        "  print(f\"Epoch: {t+1}; Train Loss: {train_loss:.3f}; Perplexity: {np.exp(train_loss):.3f}; Validation Loss: {valid_loss:.3f}; Accuracy: {accuracy:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 3.607; Perplexity: 36.869; Accuracy: 1.696\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Use the test dataset to evaluate the model\"\"\"\n",
        "\n",
        "\n",
        "dataset_test = IMDBdataset(x_test, vocab, context_size=context_size)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=128, shuffle=True)\n",
        "acc, loss = evaluate(model, num_examples, test_dataloader, loss_func)\n",
        "print(f\"Test Loss: {loss:.3f}; Perplexity: {np.exp(loss):.3f}; Accuracy: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\"\"\"Plot the training and validation losses\"\"\"\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
