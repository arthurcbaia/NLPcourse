{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxGWfhA5jxNG",
        "outputId": "92dc45e2-19e1-4b7f-f896-0c0e174bf5ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Meu nome é Arthur Baia\n"
          ]
        }
      ],
      "source": [
        "nome = 'Arthur Baia'\n",
        "print(f'Meu nome é {nome}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi"
      },
      "source": [
        "## Instruções\n",
        "\n",
        "- Treinar uma rede neural de duas camadas como classificador binário na tarefa de análise de sentimentos usando dataset IMDB usando TF-IDF como entrada.\n",
        "\n",
        "Deve-se implementar o laço de treinamento e validação da rede neural.\n",
        "\n",
        "Neste exercício usaremos o IMDB com 20k exemplos para treino, 5k para desenvolvimento e 25k para teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dfOgTUffR2"
      },
      "source": [
        "# Importando os pacotes necessários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lb8DJ6YaTtyI"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HA9p2iEUZj-"
      },
      "source": [
        "# Verificando se a GPU está disponível"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPbiUIrHZlun",
        "outputId": "7a438ec3-667e-498a-b1d3-1ce8a2b2a9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce GTX 1660 Ti\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "   print(torch. cuda. get_device_name(dev))\n",
        "else:\n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "6ddc0a6a-220f-49c6-b68b-9a6d1654cd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e validação (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "a0ee2081-dc1e-4541-c68e-78ff9a2b5fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False I don't know who wrote the script for this movie, but from the first moment on, I was irritated. Of \n",
            "False There are so many puns to play on the title of the spectacularly bad Valentine that I don't know whe\n",
            "False Unless you are already familiar with the pop stars who star in this film, save yourself the time and\n",
            "3 últimas amostras treino:\n",
            "True Why didn't Dynamo have any pants?! Where did they go?? It was never explained. That's why this movie\n",
            "False FAIL. I'd love to give this crap a 0. Yes, I registered just to rate this garbage. I want to go back\n",
            "True I had really only been exposed to Olivier's dramatic performances, and those were mostly much later \n",
            "3 primeiras amostras validação:\n",
            "True I was deeply moved by this movie in many respects. First of all, I just want to say that Clara Lago \n",
            "True Really, I liked it. The premise was good, the story fit where both respective series left off, and h\n",
            "True Terry Gilliam's stunning feature-length adaptation of Chris Marker's short film LA JETEE is full of \n",
            "3 últimas amostras validação:\n",
            "False The whole Biker Movie genre has to be made up of the worst films ever made. This one delivers a lot \n",
            "False I was watching this with one of my friends, who is a vampire freak, and I was extremely disgusted at\n",
            "False Hawked as THE MOST OFFENSIVE MOVIE EVER, GUARANTEED TO OFFEND EVERYONE- Guess what? It worked, I'm o\n"
          ]
        }
      ],
      "source": [
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_valid = x_train[n_train:]\n",
        "y_valid = y_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "y_train = y_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BN-iKb6ZlrbM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2E7CAIM5VMn"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y0MtwjKQZpde"
      },
      "outputs": [],
      "source": [
        "class TFIDF():\n",
        "\n",
        "    def __init__(self, max_vocab_token=1000):\n",
        "\n",
        "        self.max_vocab_tokens = max_vocab_token\n",
        "\n",
        "    def fit(self, corpus):\n",
        "\n",
        "        self.frequency_dict = self.create_vocab(corpus, self.max_vocab_tokens)\n",
        "        self.idf = {key: np.log10(len(corpus)/(value))\n",
        "                    for key, value in self.frequency_dict.items()}\n",
        "        \n",
        "    def transform(self, phrase):\n",
        "        unique_freq_dict = self.create_vocab(\n",
        "            [phrase], len(self.tokenize(phrase)))\n",
        "        return [unique_freq_dict[word]*idf if word in unique_freq_dict else 0 for word, idf in self.idf.items()]\n",
        "\n",
        "    def create_vocab(self, texts: List[str], max_tokens: int):\n",
        "        \"\"\"\n",
        "        Returns a dictionary whose keys are tokens and values are token ids (from 0 to max_tokens - 1).\n",
        "        \"\"\"\n",
        "        # Escreva aqui seu código.\n",
        "        L = [word for phrase in list(map(self.tokenize, texts))\n",
        "             for word in set(phrase)]\n",
        "        return dict(Counter(L).most_common(max_tokens))\n",
        "\n",
        "    def tokenize(self, text: str):\n",
        "        \"\"\"\n",
        "        Convert string to a list of tokens (i.e., words).\n",
        "        This function lower cases everything and removes punctuation.\n",
        "        \"\"\"\n",
        "        # Escreva aqui seu código.\n",
        "        return re.findall(r\"[\\w']+\", text.lower())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9cdoSKZ5l-v"
      },
      "source": [
        "# Testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTokZJ3N5ozS",
        "outputId": "79544059-0f86-4042-daf9-6a9a95282b32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_tfidf():\n",
        "    tfidf = TFIDF()\n",
        "    assert TFIDF().tokenize(\"This is a test!\") == ['this', 'is', 'a', 'test']\n",
        "    assert TFIDF().create_vocab([\"This is a test!\", \"This is another test!\"], 5) == {\n",
        "        'test': 2, 'this': 2, 'is': 2, 'a': 1, 'another': 1}\n",
        "    tfidf.fit([\"This is a test!\", \"This is another test!\"])\n",
        "    assert tfidf.idf == {'test': 0.0, 'this': 0.0, 'is': 0.0,\n",
        "                         'a': 0.3010299956639812, 'another': 0.3010299956639812}\n",
        "    tfidf.fit([\"test!\", ])\n",
        "    assert tfidf.idf == {'test': 0.0}\n",
        "    tfidf.fit([\"!\", ])\n",
        "    assert tfidf.idf == {}\n",
        "    tfidf = TFIDF(max_vocab_token=5)\n",
        "    tfidf.fit([\"This is a test!\", \"This is another test!\"])\n",
        "    assert tfidf.transform(\"This is a test!\") == [\n",
        "        0.0, 0.0, 0.0, 0.3010299956639812, 0.0]\n",
        "    return True\n",
        "\n",
        "\n",
        "test_tfidf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdZJPOXSSTdu"
      },
      "source": [
        "## IMDB dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zAvoTn4NZsas"
      },
      "outputs": [],
      "source": [
        "class IMDBdataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y, tfidf):\n",
        "        self.x = [tfidf.transform(x) for x in x]\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.x[idx]).float(), torch.tensor(self.y[idx]).long()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GwN6CVUe2iV"
      },
      "source": [
        "# IMDB dataset tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vux7tZ9hewxS",
        "outputId": "d58fc096-2b8b-4e1d-f093-c2b84aeed831"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "pytest.approx() does not support nested data structures: [0, 0.014573525916998344, 0, 0, 0, 0.043303435105349154, 0.047280187524168765, 0, 0, 0] at index 0\n  full sequence: [[0,\n  0.014573525916998344,\n  0,\n  0,\n  0,\n  0.043303435105349154,\n  0.047280187524168765,\n  0,\n  0,\n  0],\n [0, 0, 0, 0, 0, 0.043303435105349154, 0.047280187524168765, 0, 0, 0]]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39massert\u001b[39;00m pytest\u001b[39m.\u001b[39mapprox(teste_imdb\u001b[39m.\u001b[39my ,[[\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m test_imdbdataset()\n",
            "\u001b[1;32m/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb Cell 20\u001b[0m in \u001b[0;36mtest_imdbdataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tfidf_teste\u001b[39m.\u001b[39mfit(x_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m teste_imdb \u001b[39m=\u001b[39m IMDBdataset([\u001b[39m\"\u001b[39m\u001b[39mThis is a test!\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mThis is another test!\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], tfidf_teste)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39massert\u001b[39;00m pytest\u001b[39m.\u001b[39mapprox(teste_imdb\u001b[39m.\u001b[39mx ,[[\u001b[39m0\u001b[39m, \u001b[39m0.014461261606717435\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0.043519412473365916\u001b[39m, \u001b[39m0.04744970610179834\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                                                                              [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0.043519412473365916\u001b[39m, \u001b[39m0.04744970610179834\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39massert\u001b[39;00m pytest\u001b[39m.\u001b[39mapprox(teste_imdb\u001b[39m.\u001b[39my ,[[\u001b[39m0\u001b[39m],[\u001b[39m1\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arthur/Documents/machineLearning/NLPcourse/Exercicio_Aula_3_213259.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/_pytest/python_api.py:381\u001b[0m, in \u001b[0;36mApproxSequenceLike._check_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected)):\n\u001b[1;32m    380\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpytest.approx() does not support nested data structures: \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m at index \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  full sequence: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(x, index, pprint\u001b[39m.\u001b[39mpformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpected)))\n",
            "\u001b[0;31mTypeError\u001b[0m: pytest.approx() does not support nested data structures: [0, 0.014573525916998344, 0, 0, 0, 0.043303435105349154, 0.047280187524168765, 0, 0, 0] at index 0\n  full sequence: [[0,\n  0.014573525916998344,\n  0,\n  0,\n  0,\n  0.043303435105349154,\n  0.047280187524168765,\n  0,\n  0,\n  0],\n [0, 0, 0, 0, 0, 0.043303435105349154, 0.047280187524168765, 0, 0, 0]]"
          ]
        }
      ],
      "source": [
        "import pytest\n",
        "\n",
        "\n",
        "def test_imdbdataset():\n",
        "    tfidf1 = TFIDF()\n",
        "    tfidf1.fit(x_train)\n",
        "    assert IMDBdataset(x_train, y_train, tfidf1).__len__() == len(x_train)\n",
        "    assert type(IMDBdataset(x_train, y_train, tfidf1).__getitem__(0)[0]), type(\n",
        "        IMDBdataset(x_train, y_train, tfidf1).__getitem__(0)[1]) == (torch.Tensor, torch.Tensor)\n",
        "    assert IMDBdataset(x_train, y_train, tfidf1).__getitem__(0)[\n",
        "        0].shape == torch.Size([1000])\n",
        "    assert IMDBdataset(x_train, y_train, tfidf1).__getitem__(0)[\n",
        "        1].shape == torch.Size([])\n",
        "    tfidf_teste = TFIDF(max_vocab_token=10)\n",
        "    tfidf_teste.fit(x_train)\n",
        "    teste_imdb = IMDBdataset(\n",
        "        [\"This is a test!\", \"This is another test!\"], [0, 1], tfidf_teste)\n",
        "    assert pytest.approx(teste_imdb.x, [[0, 0.014461261606717435, 0, 0, 0, 0.043519412473365916, 0.04744970610179834, 0, 0, 0],\n",
        "                                        [0, 0, 0, 0, 0, 0.043519412473365916, 0.04744970610179834, 0, 0, 0]])\n",
        "    assert pytest.approx(teste_imdb.y, [[0], [1]])\n",
        "    return True\n",
        "\n",
        "\n",
        "test_imdbdataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPROuIMVe7QR"
      },
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oQ-ihSZxtUQ0"
      },
      "outputs": [],
      "source": [
        "def create_imdb_dataset(max_vocab_tokens, x, y):\n",
        "  tfidf = TFIDF(max_vocab_token=max_vocab_tokens)\n",
        "  tfidf.fit(x)\n",
        "  return IMDBdataset(x, y, tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "USXan3DNlbdX"
      },
      "outputs": [],
      "source": [
        "max_vocab_tokens = 1000\n",
        "imdb_train = create_imdb_dataset(max_vocab_tokens = max_vocab_tokens, x= x_train, y = y_train)\n",
        "imdb_validation = create_imdb_dataset(max_vocab_tokens = max_vocab_tokens, x= x_valid, y = y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NH8ZGF0HuejV"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(data, batch_size):\n",
        "  return torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W4HSQHr_jmXx"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 20\n",
        "valid_batch_size = 20\n",
        "train_dataloader = create_dataloader(imdb_train ,train_batch_size)\n",
        "valid_dataloader = create_dataloader(imdb_validation ,valid_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Yu_KU9e-m1"
      },
      "source": [
        "# Dataset loaders tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n5hQLDmEfBsd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_dataloader(max_vocab_tokens):\n",
        "    assert len(train_dataloader) == len(imdb_train)/train_batch_size\n",
        "    assert len(valid_dataloader) == len(imdb_validation)/valid_batch_size\n",
        "    # Test the shape of the first batch\n",
        "    assert train_dataloader.__iter__().__next__()[0].shape == torch.Size([\n",
        "        train_batch_size, max_vocab_tokens])\n",
        "    assert train_dataloader.__iter__().__next__(\n",
        "    )[1].shape == torch.Size([train_batch_size])\n",
        "    # Test the shape of validation batch\n",
        "    assert valid_dataloader.__iter__().__next__()[0].shape == torch.Size([\n",
        "        valid_batch_size, max_vocab_tokens])\n",
        "    assert valid_dataloader.__iter__().__next__(\n",
        "    )[1].shape == torch.Size([valid_batch_size])\n",
        "    return True\n",
        "test_dataloader(max_vocab_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_AAUDwEjJve",
        "outputId": "4a8b3f3e-fc24-4326-ff2c-754935273d5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_dataloader.__iter__().__next__()[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6VHtVZhfCM_"
      },
      "source": [
        "# Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cAHlUmTJp45V"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, input_layer_dim, k, hidden_layer_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_layer_dim, hidden_layer_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_layer_dim, k)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DxVkOt1naUJ"
      },
      "source": [
        "# Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "15qpJp4CndKc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def test_neuralnetwork(input_layer_dim, k, hidden_layer_dim):\n",
        "    model = NeuralNetwork(input_layer_dim, k, hidden_layer_dim)\n",
        "    assert model.layers[0].in_features == input_layer_dim\n",
        "    assert model.layers[0].out_features == hidden_layer_dim\n",
        "    assert model.layers[0].weight.shape == torch.Size([hidden_layer_dim , input_layer_dim])\n",
        "    assert model.layers[0].bias.shape == torch.Size([hidden_layer_dim])\n",
        "    assert model.layers[2].in_features == hidden_layer_dim\n",
        "    assert model.layers[2].out_features == k\n",
        "    assert model.layers[2].weight.shape == torch.Size([k , hidden_layer_dim])\n",
        "    assert model.layers[2].bias.shape == torch.Size([k])\n",
        "    return True\n",
        "test_neuralnetwork(1000, 2, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to train the model, which returns the train loss\"\"\"\n",
        "def train(model, train_dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x,y in train_dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss / len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Function to evaluate the model, which returns the validation loss and accuracy\"\"\"\n",
        "def evaluate(model, valid_dataloader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in valid_dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(x)\n",
        "            valid_loss += criterion(output, y).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    return valid_loss/ len(valid_dataloader), correct / len(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 20\n",
        "k = 2\n",
        "input_dim = max_vocab_tokens\n",
        "hidden_dim = 100\n",
        "learning_rate = 0.0001\n",
        "print(max_vocab_tokens)\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Nqrfh01AsBEb"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork(input_dim, k, hidden_dim).to(device)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxhKTHxTssxJ",
        "outputId": "b6976c21-3be0-49bd-f79e-e32a29a6d9db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0; Train Loss: 0.41977804984897377; Validation Loss:0.9265123593807221; Accuracy: 10.54800033569336\n",
            "Epoch: 1; Train Loss: 0.3183219897840172; Validation Loss:1.0231089532375335; Accuracy: 10.896000862121582\n",
            "Epoch: 2; Train Loss: 0.23494440852850676; Validation Loss:1.2872797409296035; Accuracy: 10.684000968933105\n",
            "Epoch: 3; Train Loss: 0.1404183025539387; Validation Loss:1.6421508547067643; Accuracy: 10.724000930786133\n",
            "Epoch: 4; Train Loss: 0.08039979126944673; Validation Loss:2.3422226833105086; Accuracy: 10.704000473022461\n",
            "Epoch: 5; Train Loss: 0.05728130178700667; Validation Loss:2.6410047409534454; Accuracy: 10.624000549316406\n",
            "Epoch: 6; Train Loss: 0.054710810285410846; Validation Loss:2.824803314447403; Accuracy: 10.496000289916992\n",
            "Epoch: 7; Train Loss: 0.04226334268059145; Validation Loss:3.113998544216156; Accuracy: 10.520000457763672\n",
            "Epoch: 8; Train Loss: 0.028155938897767557; Validation Loss:3.6398984684944153; Accuracy: 10.476000785827637\n",
            "Epoch: 9; Train Loss: 0.015505926373628881; Validation Loss:4.059815717697144; Accuracy: 10.540000915527344\n",
            "Epoch: 0; Train Loss: 0.3855733050405979; Validation Loss:0.9652248603105545; Accuracy: 10.624000549316406\n",
            "Epoch: 1; Train Loss: 0.30477530052885415; Validation Loss:0.9681032359600067; Accuracy: 10.744000434875488\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Train the model for n epochs and evaluate the model after each epoch\"\"\"\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    train_loss.append(train(model, train_dataloader, loss_func, optimizer))\n",
        "    valid_loss_, valid_acc_ = evaluate(model, valid_dataloader, loss_func)\n",
        "    valid_loss.append(valid_loss_)\n",
        "    valid_acc.append(valid_acc_)\n",
        "    print(\"Train Loss: \", train_loss[epoch])\n",
        "    print(\"Valid Loss: \", valid_loss[epoch])\n",
        "    print(\"Valid Accuracy: \", valid_acc[epoch])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
